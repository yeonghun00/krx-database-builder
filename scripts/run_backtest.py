#!/usr/bin/env python3
"""
V3 QEPM ë°±í…ŒìŠ¤íŠ¸ (Quantitative Equity Portfolio Management)

Usage:
    python3 run_backtest.py                         # ê¸°ë³¸ (quintile)
    python3 run_backtest.py --top 10                # ìƒìœ„ 10ê°œ ì¢…ëª©
    python3 run_backtest.py --qepm                  # ğŸ”¥ QEPM ëª¨ë“œ (ê¶Œì¥)
    python3 run_backtest.py --qepm --top 20         # QEPM + Top 20

QEPM ëª¨ë“œ:
    - 63ì¼(3ê°œì›”) í˜¸ë¼ì´ì¦Œ
    - Alpha íƒ€ê²Ÿ (ì‹œì¥ ëŒ€ë¹„ ì´ˆê³¼ìˆ˜ìµ)
    - ì„¹í„°ë‹¹ ìµœëŒ€ 3ì¢…ëª©
    - ë³€ë™ì„± ì—­ê°€ì¤‘ ë°°ë¶„
    - íšŒì „ìœ¨ ì œì–´ (20% ë²„í¼)
"""

import warnings
warnings.filterwarnings('ignore')

import logging
logging.basicConfig(level=logging.WARNING)

import pandas as pd
import numpy as np
import argparse
import sys
from pathlib import Path
from scipy import stats
from datetime import datetime
import time

# Add project root to Python path
sys.path.insert(0, str(Path(__file__).parent.parent))

from ml.features import FeatureEngineer
from ml.model import MLRanker

parser = argparse.ArgumentParser()
parser.add_argument('--top', type=int, default=0, help='ìƒìœ„ Nê°œ ì¢…ëª©ë§Œ (0=quintile)')
parser.add_argument('--horizon', type=int, default=63, help='ë³´ìœ  ê¸°ê°„ (ì¼): 21, 63, 126')
parser.add_argument('--slippage', type=float, default=0.5, help='í¸ë„ ìŠ¬ë¦¬í”¼ì§€ %% (ê¸°ë³¸ 0.5%%)')
parser.add_argument('--qepm', action='store_true', help='ğŸ”¥ QEPM ëª¨ë“œ (63ì¼, Alpha, ì„¹í„°ì œí•œ, ë³€ë™ì„±ê°€ì¤‘)')
parser.add_argument('--max-sector', type=int, default=3, help='ì„¹í„°ë‹¹ ìµœëŒ€ ì¢…ëª© ìˆ˜ (QEPM)')
parser.add_argument('--turnover-buffer', type=float, default=0.2, help='íšŒì „ìœ¨ ë²„í¼ (ê¸°ì¡´ ì¢…ëª© êµì²´ ê¸°ì¤€)')
parser.add_argument('--v4', action='store_true', help='ğŸ”¥ V4 ëª¨ë“œ (ë§¤í¬ë¡œ Regime Detection í”¼ì²˜ ì¶”ê°€)')
parser.add_argument('--cash-timing', action='store_true', help='ğŸ›¡ï¸ í˜„ê¸ˆ ë¹„ì¤‘ ì¡°ì ˆ (ì‹œì¥ í•˜ë½ê¸° í˜„ê¸ˆ ë³´ìœ )')
parser.add_argument('--regime-threshold', type=float, default=-0.05, help='Regime ì„ê³„ê°’ (ê¸°ë³¸ -5%%: ì´í•˜ë©´ í˜„ê¸ˆ)')
parser.add_argument('--v42', action='store_true', help='ğŸ”¥ V4.2 Market Exit (Regime<0 â†’ 100%% í˜„ê¸ˆ, ì†ì ˆ, ì ìˆ˜í•„í„°)')
parser.add_argument('--stop-loss', type=float, default=0.07, help='V4.2 ì†ì ˆ ê¸°ì¤€ (ê¸°ë³¸ 7%%)')
parser.add_argument('--score-threshold', type=float, default=0.0, help='V4.2 ìµœì†Œ ML Score (ê¸°ë³¸ 0=ë¹„í™œì„±)')
parser.add_argument('--v43', action='store_true', help='ğŸ”¥ V4.3 Residual Alpha (Beta-neutral, Purged CV, ìœ ë™ì„±í•„í„°)')
parser.add_argument('--illiquidity-filter', type=float, default=0.9, help='V4.3 ìœ ë™ì„± í•„í„° (ìƒìœ„ N%% ì œì™¸, ê¸°ë³¸ 90%%=ìƒìœ„10%% ì œì™¸)')
parser.add_argument('--v5', action='store_true', help='ğŸ”¥ V5 Research-backed 10-Feature Mode (10ê°œ í•µì‹¬ í”¼ì²˜ë§Œ)')
parser.add_argument('--v6', action='store_true', help='ğŸ”¥ V6 Quality Features (V5 + 12 fundamental quality)')
parser.add_argument('--v7', action='store_true', help='ğŸ”¥ V7 5-Pillar Only (Moat/Capital/Quality/Reinvest/Value)')
parser.add_argument('--v8', action='store_true',
    help='V8: 50 stocks, overlapping portfolios, lambdarank, hybrid target')
parser.add_argument('--v9', action='store_true',
    help='V9: Huber regression, 21d horizon, buffer zone portfolio, no hvup')
parser.add_argument('--v10', action='store_true',
    help='V10: Z-score target, LR=0.01, sector constraints, 42d horizon')
parser.add_argument('--v11', action='store_true',
    help='V11: Sector-neutral target, risk-adjusted ranking, dynamic cash, rank weighting')
parser.add_argument('--no-cache', action='store_true', help='ìºì‹œ ë¯¸ì‚¬ìš© (cold run ê°•ì œ)')
parser.add_argument('--workers', type=int, default=None, help='ë³‘ë ¬ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸: min(8, cpu_count))')
args = parser.parse_args()

# QEPM ëª¨ë“œë©´ ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ
if args.qepm:
    args.horizon = 63  # 3ê°œì›”
    if args.top == 0:
        args.top = 20  # ê¸°ë³¸ 20ì¢…ëª©

# V4.2 ëª¨ë“œ ì„¤ì • (ê³µê²©ì  Market Exit)
if args.v42:
    args.v4 = True  # V4 í”¼ì²˜ í¬í•¨
    args.cash_timing = True  # í˜„ê¸ˆ íƒ€ì´ë° í™œì„±í™”
    args.regime_threshold = 0.0  # Regime < 0ì´ë©´ í˜„ê¸ˆ (í•µì‹¬!)
    if args.top == 0:
        args.top = 20

# V4.3 ëª¨ë“œ ì„¤ì • (Residual Alpha + Purged CV)
if args.v43:
    args.v4 = True  # V4 í”¼ì²˜ í¬í•¨
    args.v42 = True  # V4.2 ê¸°ëŠ¥ í¬í•¨ (Market Exit)
    args.cash_timing = True
    args.regime_threshold = 0.0
    if args.top == 0:
        args.top = 20

# V11 ëª¨ë“œ ì„¤ì • (Sector-neutral target, risk-adjusted ranking, dynamic cash, rank weighting)
if args.v11:
    args.v10 = True  # Cascades: V10 â†’ V9 â†’ V6 â†’ V5 â†’ V4.3 â†’ V4.2 â†’ V4
    args.top = 50
    args.horizon = 42
    args.slippage = 0.2

# V10 ëª¨ë“œ ì„¤ì • (Z-score target, LR=0.01, sector constraints, 42d horizon)
if args.v10 and not args.v11:
    args.v9 = True   # Inherit V9 cascade (V6 â†’ V5 â†’ V4.3 â†’ V4.2 â†’ V4)
    args.top = 50
    args.horizon = 42  # 21 â†’ 42 days (2 months, reduces turnover cost)
    args.slippage = 0.2  # tighter slippage assumption

if args.v11:
    args.v9 = True   # Inherit V9 cascade

# V9 ëª¨ë“œ ì„¤ì • (Huber regression, 21d horizon, buffer zone, no hvup)
if args.v9:
    args.v6 = True   # Inherit V6 (NOT V8)
    if not args.v10:
        args.top = 50
        args.horizon = 21  # 21 days, not 63

# V8 ëª¨ë“œ ì„¤ì • (V6 + overlapping portfolios + lambdarank + hybrid target)
if args.v8:
    args.v6 = True  # Inherit V6 cascade (V5â†’V4.3â†’V4.2â†’V4)
    args.top = 50   # 50 stocks instead of 20
    args.horizon = 63

# V6 ëª¨ë“œ ì„¤ì • (V5 + 12 Quality Features)
if args.v6:
    args.v5 = True
    args.v43 = True
    args.v42 = True
    args.v4 = True
    args.cash_timing = True
    args.regime_threshold = 0.0
    if not args.v9:  # V9 overrides horizon to 21d
        args.horizon = 63
    if args.top == 0:
        args.top = 20

# V5 ëª¨ë“œ ì„¤ì • (Research-backed 10-Feature Mode, cascades from v43)
if args.v5:
    args.v43 = True
    args.v42 = True
    args.v4 = True
    args.cash_timing = True
    args.regime_threshold = 0.0
    if not args.v9:  # V9 overrides horizon to 21d
        args.horizon = 63  # Forced to 63 days
    if args.top == 0:
        args.top = 20

print('=' * 70)
if args.v11:
    print('ğŸ”¥ V11 ë°±í…ŒìŠ¤íŠ¸ (Sector-Neutral + Risk-Adjusted + Dynamic Cash + Rank Weighting)')
elif args.v10:
    print('ğŸ”¥ V10 ë°±í…ŒìŠ¤íŠ¸ (Z-score Target + LR=0.01 + Sector-Constrained + 42d Horizon)')
elif args.v9:
    print('ğŸ”¥ V9 ë°±í…ŒìŠ¤íŠ¸ (Huber Regression + 21d Horizon + Buffer Zone Portfolio)')
elif args.v8:
    print('ğŸ”¥ V8 ë°±í…ŒìŠ¤íŠ¸ (LambdaRank + Overlapping Portfolios + Hybrid Target)')
elif args.v7:
    print('ğŸ”¥ V7 ë°±í…ŒìŠ¤íŠ¸ (5-Pillar Only Model)')
elif args.v6:
    print('ğŸ”¥ V6 ë°±í…ŒìŠ¤íŠ¸ (V5 + 12 Fundamental Quality Features)')
elif args.v5:
    print('ğŸ”¥ V5 ë°±í…ŒìŠ¤íŠ¸ (Research-backed 10-Feature Mode)')
elif args.v43:
    print('ğŸ”¥ V4.3 ë°±í…ŒìŠ¤íŠ¸ (Residual Alpha + Beta Neutral)')
elif args.v42:
    print('ğŸ”¥ V4.2 ë°±í…ŒìŠ¤íŠ¸ (Market Exit Strategy)')
elif args.v4:
    print('ğŸ”¥ V4 ë°±í…ŒìŠ¤íŠ¸ (ë§¤í¬ë¡œ Regime Detection)')
elif args.qepm:
    print('ğŸ¦ V3 QEPM ë°±í…ŒìŠ¤íŠ¸ (ê¸°ê´€ê¸‰ í¬íŠ¸í´ë¦¬ì˜¤)')
else:
    print('ğŸš€ V3 ëª¨ë¸ ë°±í…ŒìŠ¤íŠ¸')
print(f'   {datetime.now().strftime("%Y-%m-%d %H:%M")}')
print('=' * 70)

# ============================================================================
# ì„¤ì •
# ============================================================================
HORIZON = args.horizon
TOP_N = args.top  # 0ì´ë©´ quintile ì‚¬ìš©
SLIPPAGE = args.slippage / 100  # í¸ë„ ìŠ¬ë¦¬í”¼ì§€ (0.5% -> 0.005)
ROUND_TRIP_COST = SLIPPAGE * 2  # ì™•ë³µ ë¹„ìš©
TRAIN_YEARS = 3
BUFFER_MONTHS = 2  # horizon ëŒ€ë¹„ ë²„í¼
QEPM_MODE = args.qepm
MAX_PER_SECTOR = args.max_sector
TURNOVER_BUFFER = args.turnover_buffer

print(f'\nì„¤ì •:')
print(f'  - ëª¨ë“œ: {"ğŸ¦ QEPM (ê¸°ê´€ê¸‰)" if QEPM_MODE else "ì¼ë°˜"}')
print(f'  - Target Horizon: {HORIZON}ì¼ ({HORIZON//21}ê°œì›”)')
print(f'  - í¬íŠ¸í´ë¦¬ì˜¤: {"ìƒìœ„ " + str(TOP_N) + "ê°œ" if TOP_N > 0 else "Quintile (ìƒìœ„ 20%)"}')
print(f'  - ìŠ¬ë¦¬í”¼ì§€: {args.slippage}% (ì™•ë³µ {args.slippage*2}%)')
if QEPM_MODE:
    print(f'  - ì„¹í„° ì œí•œ: ì„¹í„°ë‹¹ ìµœëŒ€ {MAX_PER_SECTOR}ì¢…ëª©')
    print(f'  - íšŒì „ìœ¨ ë²„í¼: {TURNOVER_BUFFER*100:.0f}% (êµì²´ ê¸°ì¤€)')
    print(f'  - íƒ€ê²Ÿ: Alpha (ì‹œì¥ ëŒ€ë¹„ ì´ˆê³¼ìˆ˜ìµ)')
    print(f'  - ê°€ì¤‘ì¹˜: ë³€ë™ì„± ì—­ê°€ì¤‘ (Risk Parity)')
print(f'  - í•™ìŠµ ê¸°ê°„: {TRAIN_YEARS}ë…„')
if args.v11:
    print(f'  - ğŸ”¥ V11 Final Alpha Build:')
    print(f'      â€¢ Target: Sector-neutral Z-score')
    print(f'      â€¢ Ranking: pred_score / volatility_20d (risk-adjusted)')
    print(f'      â€¢ Weighting: Rank-based (top10=3x, 11-30=2x, 31-50=1x)')
    print(f'      â€¢ Cash: VKOSPI absolute + regime graduated')
    print(f'      â€¢ Slippage Ã—: 0.3 (rank weighting reduces tail churn)')
    print(f'      â€¢ Everything else: inherited from V10')
elif args.v10:
    print(f'  - ğŸ”¥ V10 Underfitting/Slippage/Sector Fix:')
    print(f'      â€¢ í”¼ì²˜: V9 23ê°œ (inherited)')
    print(f'      â€¢ Target: Cross-sectional Z-score (relative ranking)')
    print(f'      â€¢ Model: Huber (delta=1.0), LR=0.01, 1000 trees, patience=100')
    print(f'      â€¢ Portfolio: 50 stocks, sector max 7, buffer zone (hold<=100)')
    print(f'      â€¢ Horizon: 42d, rebalance every 42d')
    print(f'      â€¢ Validation: Shuffled 10% (not time-based)')
    print(f'      â€¢ Risk: VKOSPI spike de-grossing (inherited)')
elif args.v9:
    print(f'  - ğŸ”¥ V9 Recovery Build:')
    print(f'      â€¢ í”¼ì²˜: V6 22ê°œ + reversal 1ê°œ = 23ê°œ (no hvup)')
    print(f'      â€¢ Target: Signed Log Return (continuous, robust)')
    print(f'      â€¢ Model: Huber Regression + Early Stopping')
    print(f'      â€¢ Portfolio: 50 stocks, buffer zone (buy<=50, hold<=120)')
    print(f'      â€¢ Horizon: 21d (Korean theme-driven market)')
    print(f'      â€¢ Risk: VKOSPI spike de-grossing')
elif args.v8:
    print(f'  - ğŸ”¥ V8 IC-to-Return Gap Fix:')
    print(f'      â€¢ í”¼ì²˜: V6 22ê°œ + 3ê°œ = 25ê°œ')
    print(f'      â€¢ Target: Hybrid Rank (70% residual + 30% absolute)')
    print(f'      â€¢ Model: LambdaRank + Early Stopping')
    print(f'      â€¢ Portfolio: 50 stocks, 3 overlapping cohorts (21d stagger)')
    print(f'      â€¢ Risk: VKOSPI spike de-grossing')
elif args.v6:
    print(f'  - ğŸ”¥ V6 Quality Features (5-Pillar Framework):')
    print(f'      â€¢ í”¼ì²˜: V5 10ê°œ + Quality 12ê°œ = 22ê°œ')
    print(f'      â€¢ Pillars: Moat, Capital, Quality, Growth, Value')
    print(f'      â€¢ V5 ê¸°ëŠ¥ ëª¨ë‘ ìƒì†')
elif args.v7:
    print(f'  - ğŸ”¥ V7 5-Pillar Only:')
    print(f'      â€¢ í”¼ì²˜: 5ê°œ í•©ì„± ìŠ¤ì½”ì–´ë§Œ ì‚¬ìš©')
    print(f'      â€¢ Pillars: Moat, Capital, Earnings, Reinvestment, Value')
elif args.v5:
    print(f'  - ğŸ”¥ V5 Research-backed 10-Feature Mode:')
    print(f'      â€¢ í”¼ì²˜: 10ê°œ í•µì‹¬ í”¼ì²˜ë§Œ ì‚¬ìš©')
    print(f'      â€¢ Horizon: 63ì¼ (forced)')
    print(f'      â€¢ Target: Residual Return (Beta-adjusted)')
    print(f'      â€¢ Purged CV + Market Exit + Macro ìƒì†')
elif args.v43:
    print(f'  - ğŸ”¥ V4.3 Residual Alpha:')
    print(f'      â€¢ Target: Residual Return (Beta-adjusted)')
    print(f'      â€¢ Purged CV: {HORIZON}ì¼ ê²¹ì¹¨ ì œê±°')
    print(f'      â€¢ ìœ ë™ì„± í•„í„°: Amihud ìƒìœ„ {(1-args.illiquidity_filter)*100:.0f}% ì œì™¸')
    print(f'      â€¢ Regime < {args.regime_threshold*100:.0f}% â†’ 100% í˜„ê¸ˆ')
elif args.v42:
    print(f'  - ğŸ”¥ V4.2 Market Exit:')
    print(f'      â€¢ Regime < {args.regime_threshold*100:.0f}% â†’ 100% í˜„ê¸ˆ')
    print(f'      â€¢ ì†ì ˆ ê¸°ì¤€: -{args.stop_loss*100:.0f}% (21ì¼ ë‚´)')
    if args.score_threshold > 0:
        print(f'      â€¢ ìµœì†Œ ML Score: {args.score_threshold}')
elif args.cash_timing:
    print(f'  - ğŸ›¡ï¸ í˜„ê¸ˆ íƒ€ì´ë°: ON (Regime < {args.regime_threshold*100:.0f}% â†’ í˜„ê¸ˆ)')

# ============================================================================
# ë°ì´í„° ë¡œë“œ
# ============================================================================
print('\n[1/3] ë°ì´í„° ë¡œë“œ ì¤‘...')
stage_t0 = time.time()

fe = FeatureEngineer('krx_stock_data.db')
df = fe.prepare_ml_data(
    start_date='20110101',  # ì „ì²´ ê¸°ê°„
    end_date='20260128',
    target_horizon=HORIZON,
    min_market_cap=500_000_000_000,
    include_fundamental=True,
    include_macro=args.v4,  # V4: ë§¤í¬ë¡œ í”¼ì²˜ ì¶”ê°€
    use_cache=not args.no_cache,
    n_workers=args.workers
)
print(f'  â± [1/3] ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {time.time()-stage_t0:.1f}s')

# ============================================================================
# í˜„ê¸ˆ íƒ€ì´ë°ìš© ì‹œì¥ Regime ë°ì´í„° ë¡œë“œ (ìµœì í™”)
# ============================================================================
MARKET_REGIME = {}
if args.cash_timing:
    regime_t0 = time.time()
    import sqlite3
    conn = sqlite3.connect('krx_stock_data.db')
    # ë‹¨ìˆœ ì¿¼ë¦¬ë¡œ ë°ì´í„° ë¡œë“œ í›„ pandasì—ì„œ ê³„ì‚° (ë” ë¹ ë¦„)
    regime_query = '''
    SELECT date, closing_index
    FROM index_daily_prices
    WHERE index_code = 'KOSPI_ì½”ìŠ¤í”¼_200'
    ORDER BY date
    '''
    regime_df = pd.read_sql_query(regime_query, conn)
    conn.close()

    # Pandasì—ì„œ MA ê³„ì‚° (SQL windowë³´ë‹¤ ë¹ ë¦„)
    regime_df['ma_120'] = regime_df['closing_index'].rolling(120, min_periods=60).mean()
    regime_df['regime_score'] = (regime_df['closing_index'] / regime_df['ma_120']) - 1
    MARKET_REGIME = dict(zip(regime_df['date'], regime_df['regime_score']))

    # ì—°ë„ë³„ í‰ê·  regime í‘œì‹œ
    regime_df['year'] = regime_df['date'].str[:4]
    yearly_regime = regime_df.groupby('year')['regime_score'].mean()
    print(f'\n  [ì‹œì¥ Regime by Year]')
    for year, score in yearly_regime.items():
        if pd.notna(score):
            status = 'ğŸŸ¢' if score > 0 else 'ğŸ”´'
            print(f'    {year}: {score*100:+.1f}% {status}')
    print(f'  â± Market regime load: {time.time()-regime_t0:.1f}s')

def should_hold_cash(date, threshold):
    """í˜„ê¸ˆ ë³´ìœ  ì—¬ë¶€ ê²°ì •"""
    if not MARKET_REGIME:
        return False
    regime = MARKET_REGIME.get(date, 0)
    return regime < threshold

# í”¼ì²˜ ë¶„ë¥˜
momentum_features = [c for c in fe.MOMENTUM_FEATURES if c in df.columns]
volume_features = [c for c in fe.VOLUME_FEATURES if c in df.columns]
volatility_features = [c for c in fe.VOLATILITY_FEATURES if c in df.columns]
intuition_features = [c for c in fe.INTUITION_FEATURES if c in df.columns]
traditional_features = [c for c in fe.TRADITIONAL_FEATURES if c in df.columns]
fund_features = [c for c in fe.FUNDAMENTAL_FEATURES if c in df.columns]
macro_features = [c for c in fe.MACRO_FEATURES if c in df.columns] if args.v4 else []

all_features = (momentum_features + volume_features + volatility_features +
                intuition_features + traditional_features + fund_features + macro_features)

# V9: Override with 23 features (V6 22 + reversal, no hvup)
if args.v9:
    all_features = [c for c in fe.V9_FEATURES if c in df.columns]
    missing_v9 = [c for c in fe.V9_FEATURES if c not in df.columns]
    if missing_v9:
        print(f'  âš ï¸ Missing V9 features: {missing_v9}')
# V8: Override with 25 features (V6 22 + 3 new)
elif args.v8:
    all_features = [c for c in fe.V8_FEATURES if c in df.columns]
    missing_v8 = [c for c in fe.V8_FEATURES if c not in df.columns]
    if missing_v8:
        print(f'  âš ï¸ Missing V8 features: {missing_v8}')
# V6: Override with 22 features (V5 10 + Quality 12)
elif args.v6:
    all_features = [c for c in fe.V6_FEATURES if c in df.columns]
    missing_v6 = [c for c in fe.V6_FEATURES if c not in df.columns]
    if missing_v6:
        print(f'  âš ï¸ Missing V6 features: {missing_v6}')
# V5: Override with 10 focused features
elif args.v5:
    all_features = [c for c in fe.V5_FEATURES if c in df.columns]
    missing_v5 = [c for c in fe.V5_FEATURES if c not in df.columns]
    if missing_v5:
        print(f'  âš ï¸ Missing V5 features: {missing_v5}')
elif args.v7:
    all_features = [c for c in fe.MODEL7_FEATURES if c in df.columns]
    missing_v7 = [c for c in fe.MODEL7_FEATURES if c not in df.columns]
    if missing_v7:
        print(f'  âš ï¸ Missing V7 features: {missing_v7}')

tech_count = len(momentum_features + volume_features + volatility_features +
                 intuition_features + traditional_features)

print(f'  ì´ ë°ì´í„°: {len(df):,} rows')
if args.v9:
    v6_in_use = [c for c in fe.V6_FEATURES if c in df.columns]
    v9_new = [c for c in ['short_term_reversal'] if c in df.columns]
    print(f'  í”¼ì²˜ êµ¬ì„±: V9 (V6 {len(v6_in_use)} + reversal {len(v9_new)} = {len(all_features)}, no hvup)')
    print(f'  ì´ í”¼ì²˜: {len(all_features)}ê°œ')
elif args.v8:
    v8_new = ['inventory_sales_gap', 'current_ratio', 'hvup_ratio']
    v8_in_use = [c for c in v8_new if c in df.columns]
    v6_in_use = [c for c in fe.V6_FEATURES if c in df.columns]
    print(f'  í”¼ì²˜ êµ¬ì„±: V8 (V6 {len(v6_in_use)} + New {len(v8_in_use)} = {len(all_features)})')
    print(f'    [V8 New]:')
    for f_name in v8_in_use:
        print(f'      - {f_name}')
    print(f'  ì´ í”¼ì²˜: {len(all_features)}ê°œ')
elif args.v6:
    v5_in_use = [c for c in fe.V5_FEATURES if c in df.columns]
    quality_in_use = [c for c in fe.QUALITY_FEATURES if c in df.columns]
    print(f'  í”¼ì²˜ êµ¬ì„±: V6 (V5 {len(v5_in_use)} + Quality {len(quality_in_use)} = {len(all_features)})')
    print(f'    [V5 Base]:')
    for f_name in v5_in_use:
        print(f'      - {f_name}')
    print(f'    [Quality]:')
    for f_name in quality_in_use:
        print(f'      - {f_name}')
    print(f'  ì´ í”¼ì²˜: {len(all_features)}ê°œ')
elif args.v5:
    print(f'  í”¼ì²˜ êµ¬ì„±: V5 (10-Feature Mode)')
    for f_name in all_features:
        print(f'    - {f_name}')
    print(f'  ì´ í”¼ì²˜: {len(all_features)}ê°œ')
else:
    print(f'  í”¼ì²˜ êµ¬ì„±:')
    print(f'    - ëª¨ë©˜í…€: {len(momentum_features)}ê°œ')
    print(f'    - ìˆ˜ê¸‰: {len(volume_features)}ê°œ')
    print(f'    - ë³€ë™ì„±: {len(volatility_features)}ê°œ')
    print(f'    - ë³¸ëŠ¥ì „ëµ: {len(intuition_features)}ê°œ')
    print(f'    - ì „í†µì§€í‘œ: {len(traditional_features)}ê°œ')
    print(f'    - ì¬ë¬´: {len(fund_features)}ê°œ')
    if args.v4:
        print(f'    - ğŸ”¥ ë§¤í¬ë¡œ: {len(macro_features)}ê°œ')
    print(f'  ì¬ë¬´ ë¹„ì¤‘: {len(fund_features)/len(all_features)*100:.1f}%')

# Forward return ì¶”ê°€
df = df.sort_values(['stock_code', 'date'])
df['year'] = df['date'].str[:4].astype(int)
years = sorted(df['year'].unique())

# ============================================================================
# Walk-Forward ë°±í…ŒìŠ¤íŠ¸
# ============================================================================
stage_t0 = time.time()
print('\n[2/3] Walk-Forward ë°±í…ŒìŠ¤íŠ¸...')
print('-' * 70)

# íƒ€ê²Ÿ ì„¤ì •: V11 > V10 > V9 > V8 > V4.3 > QEPM > ì¼ë°˜
if args.v11:
    target_col = f'target_sector_zscore_{HORIZON}d'
    return_col = f'forward_return_{HORIZON}d'
elif args.v10:
    target_col = f'target_zscore_{HORIZON}d'
    return_col = f'forward_return_{HORIZON}d'
elif args.v9:
    target_col = f'target_log_return_{HORIZON}d'
    return_col = f'forward_return_{HORIZON}d'
elif args.v8:
    target_col = f'target_hybrid_rank_{HORIZON}d'
    return_col = f'forward_return_{HORIZON}d'
    if target_col not in df.columns:
        print(f'  âš ï¸ Hybrid target ì—†ìŒ, residual rankë¡œ fallback')
        target_col = f'target_residual_rank_{HORIZON}d'
elif args.v43:
    # ğŸ”¥ V4.3: Residual Return (Beta-adjusted) íƒ€ê²Ÿ
    target_col = f'target_residual_rank_{HORIZON}d'
    return_col = f'forward_return_{HORIZON}d'  # ì‹¤ì œ ìˆ˜ìµì€ raw return
    if target_col not in df.columns:
        print(f'  âš ï¸ Residual target ì—†ìŒ, Alphaë¡œ fallback')
        target_col = f'target_alpha_rank_{HORIZON}d'
elif QEPM_MODE:
    target_col = f'target_alpha_rank_{HORIZON}d'  # Alpha ìˆœìœ„
    return_col = f'forward_alpha_{HORIZON}d'      # Alpha ìˆ˜ìµë¥ 
    if target_col not in df.columns:
        target_col = f'target_rank_{HORIZON}d'    # fallback
        return_col = f'forward_return_{HORIZON}d'
else:
    target_col = f'target_rank_{HORIZON}d'
    return_col = f'forward_return_{HORIZON}d'

# V4.3: ìœ ë™ì„± í•„í„° ì ìš©
if args.v43 and 'amihud_rank' in df.columns:
    original_len = len(df)
    df = df[df['amihud_rank'] <= args.illiquidity_filter]
    filtered = original_len - len(df)
    print(f'  ìœ ë™ì„± í•„í„°: {filtered:,}ê°œ ì œì™¸ (Amihud ìƒìœ„ {(1-args.illiquidity_filter)*100:.0f}%)')

all_results = []
all_test_dfs = {}  # year -> test_df for decile/sector analysis

# ============================================================================
# QEPM í—¬í¼ í•¨ìˆ˜ë“¤
# ============================================================================

def load_all_benchmark_returns(horizon=63):
    """
    ëª¨ë“  ì—°ë„ì˜ ë²¤ì¹˜ë§ˆí¬ ìˆ˜ìµë¥ ì„ í•œ ë²ˆì— ë¡œë“œ (ìºì‹œìš©)
    Uses index_daily_prices table with KOSPI index.
    Computes forward returns from closing_index via pandas shift.
    """
    import sqlite3

    try:
        conn = sqlite3.connect('krx_stock_data.db')
        query = """
        SELECT date, closing_index
        FROM index_daily_prices
        WHERE index_code = 'KOSPI_ì½”ìŠ¤í”¼'
          AND closing_index IS NOT NULL
        ORDER BY date
        """
        bm_df = pd.read_sql_query(query, conn)
        conn.close()

        if len(bm_df) == 0:
            print('    âš ï¸ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ì—†ìŒ (KOSPI_ì½”ìŠ¤í”¼)')
            return {}

        # Compute forward return over horizon trading days
        bm_df['forward_price'] = bm_df['closing_index'].shift(-horizon)
        bm_df['forward_return'] = (bm_df['forward_price'] / bm_df['closing_index'] - 1) * 100
        bm_df['year'] = bm_df['date'].str[:4].astype(int)

        # Average forward return per year
        yearly = bm_df.dropna(subset=['forward_return']).groupby('year')['forward_return'].mean()
        return yearly.to_dict()
    except Exception as e:
        print(f'    âš ï¸ ë²¤ì¹˜ë§ˆí¬ ë¡œë“œ ì‹¤íŒ¨: {e}')
        return {}

# ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ë¯¸ë¦¬ ë¡œë“œ (í•œ ë²ˆë§Œ)
_BENCHMARK_CACHE = None

def get_benchmark_return(year, horizon=63):
    """ìºì‹œëœ ë²¤ì¹˜ë§ˆí¬ ìˆ˜ìµë¥  ë°˜í™˜"""
    global _BENCHMARK_CACHE
    if _BENCHMARK_CACHE is None:
        _BENCHMARK_CACHE = load_all_benchmark_returns(horizon)
    return _BENCHMARK_CACHE.get(year, 0)

def select_with_sector_constraint(df, top_n, max_per_sector):
    """ì„¹í„° ì œì•½ ì ìš©í•œ ì¢…ëª© ì„ ì • (ë²¡í„°í™” ë²„ì „)"""
    if len(df) == 0:
        return df

    df = df.sort_values('pred_rank').copy()
    df['sector'] = df['sector'].fillna('Unknown')

    # ì„¹í„°ë³„ ëˆ„ì  ì¹´ìš´íŠ¸ ê³„ì‚° (ë²¡í„°í™”)
    df['_sector_cumcount'] = df.groupby('sector').cumcount() + 1

    # ì„¹í„°ë‹¹ max_per_sector ì´í•˜ì¸ í–‰ë§Œ ì„ íƒ
    eligible = df[df['_sector_cumcount'] <= max_per_sector]

    # ìƒìœ„ top_nê°œ ì„ íƒ
    result = eligible.head(top_n).drop(columns=['_sector_cumcount'])
    return result

def apply_inverse_volatility_weight(df, return_col):
    """ë³€ë™ì„± ì—­ê°€ì¤‘ ì ìš©í•œ ìˆ˜ìµë¥  ê³„ì‚°"""
    if 'volatility_20d' not in df.columns or len(df) == 0:
        return df[return_col].mean()

    vol = df['volatility_20d'].fillna(df['volatility_20d'].median())
    vol = vol.clip(lower=0.1)  # ìµœì†Œ ë³€ë™ì„±
    inv_vol = 1 / vol
    weights = inv_vol / inv_vol.sum()

    return (df[return_col] * weights).sum()

# Parallel walk-forward: train all years concurrently
from concurrent.futures import ThreadPoolExecutor

def _train_year(test_year):
    """Train and predict for one test year (thread-safe)."""
    train_start = test_year - TRAIN_YEARS
    train_end = test_year - 1
    buffer_month = 12 - (HORIZON // 21 + BUFFER_MONTHS)
    buffer_month = max(1, min(buffer_month, 10))
    train_cutoff = f'{train_end}{buffer_month:02d}01'

    train_df = df[(df['year'] >= train_start) &
                  (df['year'] <= train_end) &
                  (df['date'] <= train_cutoff)].copy()
    test_df = df[df['year'] == test_year].copy()

    if len(train_df) < 1000 or len(test_df) < 100:
        return None

    if args.v43:
        test_start_date = test_df['date'].min()
        purge_cutoff = pd.to_datetime(test_start_date, format='%Y%m%d') - pd.Timedelta(days=HORIZON * 1.5)
        purge_cutoff_str = purge_cutoff.strftime('%Y%m%d')
        train_df = train_df[train_df['date'] <= purge_cutoff_str]
        if 'macro_risk_score' in train_df.columns:
            train_df['sample_weight'] = 1.0
            high_risk_mask = train_df['macro_risk_score'] > 0.6
            train_df.loc[high_risk_mask, 'sample_weight'] = 2.0
        else:
            train_df['sample_weight'] = 1.0

    if args.v11:
        # V11: Sector-neutral target, same training as V10 but with V11_PARAMS
        _model = MLRanker(
            feature_cols=all_features,
            target_col=target_col,
            model_type='regressor',
            time_decay=0.7,
            patience=100
        )
        lgb_params = {**MLRanker.V11_PARAMS, 'n_jobs': 1}

        _shuffled = train_df.sample(frac=1.0, random_state=42)
        val_size = max(int(len(_shuffled) * 0.1), 100)
        val_df = _shuffled.iloc[:val_size]
        train_only = _shuffled.iloc[val_size:]

        if len(val_df) > 100:
            _model.train(train_only, val_df=val_df, params=lgb_params)
        else:
            _model.train(train_only, params=lgb_params)
    elif args.v10:
        # V10: Huber regression on Z-score target, shuffled validation, patience=100
        _model = MLRanker(
            feature_cols=all_features,
            target_col=target_col,
            model_type='regressor',
            time_decay=0.7,
            patience=100
        )
        lgb_params = {**MLRanker.V10_PARAMS, 'n_jobs': 1}

        # Validation: shuffled 10% of train (not time-based split)
        # Fixes underfitting caused by temporal distribution shift
        _shuffled = train_df.sample(frac=1.0, random_state=42)
        val_size = max(int(len(_shuffled) * 0.1), 100)
        val_df = _shuffled.iloc[:val_size]
        train_only = _shuffled.iloc[val_size:]

        if len(val_df) > 100:
            _model.train(train_only, val_df=val_df, params=lgb_params)
        else:
            _model.train(train_only, params=lgb_params)
    elif args.v9:
        # V9: Huber regression with early stopping
        _model = MLRanker(
            feature_cols=all_features,
            target_col=target_col,
            model_type='regressor',
            time_decay=0.7
        )
        lgb_params = {**MLRanker.V9_HUBER_PARAMS, 'n_jobs': 1}
        val_cutoff = f'{train_end}0801'
        val_df = train_df[train_df['date'] > val_cutoff]
        train_only = train_df[train_df['date'] <= val_cutoff]
        if len(val_df) > 100:
            _model.train(train_only, val_df=val_df, params=lgb_params)
        else:
            _model.train(train_only, params=lgb_params)
    elif args.v8:
        # V8: LambdaRank with validation + early stopping
        # LambdaRank requires integer relevance labels (0-4)
        # Convert continuous percentile rank (0-1) â†’ 5 relevance grades
        # Drop rows with NaN target first (no forward return data)
        _v8_target = f'_v8_relevance'
        train_df = train_df.dropna(subset=[target_col]).copy()
        train_df[_v8_target] = (train_df[target_col] * 4).round().astype(int).clip(0, 4)

        _model = MLRanker(
            feature_cols=all_features,
            target_col=_v8_target,
            model_type='ranker',
            time_decay=0.0  # lambdarank doesn't use sample weights
        )

        # Split: last 4 months of training â†’ validation
        val_cutoff = f'{train_end}0801'
        val_df = train_df[train_df['date'] > val_cutoff].copy()
        train_only = train_df[train_df['date'] <= val_cutoff].copy()

        lgb_params = {**MLRanker.V8_RANK_PARAMS, 'n_jobs': 1}
        if len(val_df) > 100:
            _model.train(train_only, val_df=val_df, params=lgb_params)
        else:
            _model.train(train_only, params=lgb_params)
    else:
        # n_jobs=1 per model to avoid thread contention
        lgb_params = {**MLRanker.REGRESSION_PARAMS, 'n_jobs': 1}
        _model = MLRanker(
            feature_cols=all_features,
            target_col=target_col,
            model_type='regressor',
            time_decay=0.5 if QEPM_MODE else 0.7
        )

        if args.v43 and 'sample_weight' in train_df.columns:
            _model.train(train_df, params=lgb_params, sample_weight=train_df['sample_weight'].values)
        else:
            _model.train(train_df, params=lgb_params)

    test_df['pred_score'] = _model.predict(test_df)
    test_df['pred_rank'] = test_df.groupby('date')['pred_score'].rank(ascending=False)

    return (test_year, _model, test_df, len(train_df), len(test_df))

# Pre-populate benchmark cache before parallel execution
if QEPM_MODE and years:
    get_benchmark_return(years[0], HORIZON)

with ThreadPoolExecutor() as pool:
    _trained = [r for r in pool.map(_train_year, years) if r is not None]

for test_year, model, test_df, _n_train, _n_test in _trained:
    avg_turnover = 0.0  # default, overridden in V9/V10/V11

    # ğŸ”¥ V4.2: ML Score ì •ê·œí™” (0~1 ë²”ìœ„ë¡œ) ë° ì„ê³„ê°’ í•„í„°
    score_filtered = False
    if args.v42 and args.score_threshold > 0:
        # ë‚ ì§œë³„ ì ìˆ˜ ì •ê·œí™” (percentile)
        test_df['pred_score_pct'] = test_df.groupby('date')['pred_score'].rank(pct=True)
        # ì„ê³„ê°’ ë¯¸ë‹¬ ì¢…ëª© í•„í„° (pred_rankì„ ë†’ì—¬ì„œ ì„ íƒ ì•ˆë˜ê²Œ)
        low_score_mask = test_df['pred_score_pct'] < args.score_threshold
        if low_score_mask.any():
            score_filtered = True

    if TOP_N > 0:
        if args.v11:
            # V11: Sector-neutral + risk-adjusted ranking + rank weighting + dynamic cash
            test_df = test_df.sort_values(['date', 'pred_rank']).copy()
            test_df['sector'] = test_df['sector'].fillna('Unknown')
            dates = sorted(test_df['date'].unique())

            hold_threshold = int(TOP_N * 2.0)  # 100
            MAX_SECTOR = 7
            rebal_interval = 42

            rebal_dates = dates[::rebal_interval]

            # Pre-compute daily VKOSPI absolute level for dynamic cash
            _v11_vkospi = {}
            if 'fear_index_level' in test_df.columns:
                fear_by_date = test_df.groupby('date')['fear_index_level'].first().sort_index()
                for d in fear_by_date.index:
                    if pd.notna(fear_by_date[d]):
                        _v11_vkospi[d] = fear_by_date[d]

            prev_portfolio = set()
            cohort_picks = {}
            cohort_weights = {}  # stock_code -> weight
            turnover_list = []

            for rd in rebal_dates:
                day_df = test_df[test_df['date'] == rd].copy()

                # Risk-adjusted ranking: pred_score / volatility_20d
                if 'volatility_20d' in day_df.columns:
                    vol = day_df['volatility_20d'].fillna(day_df['volatility_20d'].median()).clip(lower=0.05)
                    day_df['risk_adj_score'] = day_df['pred_score'] / vol
                else:
                    day_df['risk_adj_score'] = day_df['pred_score']
                day_df['risk_adj_rank'] = day_df['risk_adj_score'].rank(ascending=False)
                day_df = day_df.sort_values('risk_adj_rank')

                # Survivors: existing holdings still within hold threshold
                if prev_portfolio:
                    hold_df = day_df[day_df['stock_code'].isin(prev_portfolio)]
                    survivors = set(hold_df[hold_df['risk_adj_rank'] <= hold_threshold]['stock_code'].tolist())
                else:
                    survivors = set()

                # New buys: top-ranked, sector-constrained
                day_df['_sc'] = day_df.groupby('sector').cumcount() + 1
                eligible = day_df[day_df['_sc'] <= MAX_SECTOR]
                new_buys = set(eligible.head(TOP_N)['stock_code'].tolist())

                portfolio = new_buys | survivors
                cohort_picks[rd] = portfolio

                # Turnover tracking
                if prev_portfolio:
                    buys = portfolio - prev_portfolio
                    sells = prev_portfolio - portfolio
                    denom = max(len(portfolio), 1)
                    turnover_list.append((len(buys) + len(sells)) / (2 * denom))

                # Rank-based weighting: top10=3x, 11-30=2x, 31-50=1x
                weights = {}
                for _, row in eligible.head(TOP_N).iterrows():
                    r = row['risk_adj_rank']
                    if r <= 10:
                        w = 3.0
                    elif r <= 30:
                        w = 2.0
                    else:
                        w = 1.0
                    weights[row['stock_code']] = w
                # Survivors get weight 1.0 if not already in new_buys
                for sc in survivors:
                    if sc not in weights:
                        weights[sc] = 1.0
                # Normalize weights
                total_w = sum(weights.values())
                if total_w > 0:
                    weights = {k: v / total_w for k, v in weights.items()}
                cohort_weights[rd] = weights

                prev_portfolio = portfolio
            avg_turnover = np.mean(turnover_list) if turnover_list else 0

            # Compute daily returns with rank weighting + dynamic cash
            daily_returns = []
            v11_cash_days = 0
            v11_degross_days = 0
            for d in dates:
                active_rebals = [rd for rd in rebal_dates if rd <= d]
                if not active_rebals:
                    continue
                latest_rebal = active_rebals[-1]
                weights = cohort_weights.get(latest_rebal, {})

                day_df = test_df[test_df['date'] == d]
                port_df = day_df[day_df['stock_code'].isin(weights.keys())]

                if len(port_df) == 0:
                    continue

                # Weighted return
                port_df = port_df.copy()
                port_df['_w'] = port_df['stock_code'].map(weights).fillna(0)
                w_sum = port_df['_w'].sum()
                if w_sum > 0:
                    port_df['_w'] = port_df['_w'] / w_sum
                raw_ret = (port_df[return_col] * port_df['_w']).sum()

                # Dynamic cash: VKOSPI absolute level + regime graduated
                exposure = 1.0
                regime_bad = MARKET_REGIME.get(d, 0) < args.regime_threshold if MARKET_REGIME else False
                vkospi_level = _v11_vkospi.get(d, 0)
                vkospi_high = vkospi_level > 25

                if regime_bad and vkospi_high:
                    exposure = 0.3   # 70% cash
                    v11_cash_days += 1
                elif regime_bad:
                    exposure = 0.5   # 50% cash
                    v11_degross_days += 1
                elif vkospi_high:
                    exposure = 0.7   # 30% cash
                    v11_degross_days += 1

                daily_returns.append(raw_ret * exposure)

            q5_raw = np.mean(daily_returns) * 100 if daily_returns else 0
            top_df = test_df[test_df['pred_rank'] <= TOP_N]

            # Track cash/degross for reporting
            total_days = len(daily_returns) if daily_returns else 1
            cash_ratio = v11_cash_days / total_days
            held_cash = (v11_cash_days + v11_degross_days) > 0

            # Bottom N
            max_rank = test_df.groupby('date')['pred_rank'].transform('max')
            bottom_df = test_df[test_df['pred_rank'] > max_rank - TOP_N]
            q1_raw = bottom_df[return_col].mean() * 100

            # Lower effective slippage: rank weighting reduces tail churn
            effective_slippage = ROUND_TRIP_COST * 0.3
            q5 = q5_raw - effective_slippage * 100
            q1 = q1_raw - effective_slippage * 100
            spread = q5 - q1 if not (np.isnan(q5) or np.isnan(q1)) else np.nan

        elif args.v10:
            # V10: Sector-constrained buffer zone portfolio
            # Max 7 stocks per sector, 42-day rebalance, tighter hold threshold
            test_df = test_df.sort_values(['date', 'pred_rank']).copy()
            test_df['sector'] = test_df['sector'].fillna('Unknown')
            dates = sorted(test_df['date'].unique())

            hold_threshold = int(TOP_N * 2.0)  # 100 (tighter than V9's 120)
            MAX_SECTOR = 7
            rebal_interval = 42  # match horizon

            rebal_dates = dates[::rebal_interval]

            # Pre-compute daily VKOSPI spike signal for this year
            _v10_fear_spike = {}
            if 'fear_index_level' in test_df.columns:
                fear_by_date = test_df.groupby('date')['fear_index_level'].first().sort_index()
                fear_ma20 = fear_by_date.rolling(20, min_periods=10).mean()
                for d in fear_by_date.index:
                    if pd.notna(fear_by_date[d]) and pd.notna(fear_ma20.get(d)):
                        _v10_fear_spike[d] = fear_by_date[d] > (fear_ma20[d] * 1.2)
                    else:
                        _v10_fear_spike[d] = False

            prev_portfolio = set()
            cohort_picks = {}
            turnover_list = []

            for rd in rebal_dates:
                day_df = test_df[test_df['date'] == rd].copy()
                day_df = day_df.sort_values('pred_rank')

                # Survivors: existing holdings still within hold threshold
                if prev_portfolio:
                    hold_df = day_df[day_df['stock_code'].isin(prev_portfolio)]
                    survivors = set(hold_df[hold_df['pred_rank'] <= hold_threshold]['stock_code'].tolist())
                else:
                    survivors = set()

                # New buys: top-ranked, sector-constrained
                day_df['_sc'] = day_df.groupby('sector').cumcount() + 1
                eligible = day_df[day_df['_sc'] <= MAX_SECTOR]
                new_buys = set(eligible.head(TOP_N)['stock_code'].tolist())

                portfolio = new_buys | survivors
                cohort_picks[rd] = portfolio

                # Turnover tracking
                if prev_portfolio:
                    buys = portfolio - prev_portfolio
                    sells = prev_portfolio - portfolio
                    denom = max(len(portfolio), 1)
                    turnover_list.append((len(buys) + len(sells)) / (2 * denom))

                prev_portfolio = portfolio
            avg_turnover = np.mean(turnover_list) if turnover_list else 0

            # Compute daily returns with per-day regime/VKOSPI gross exposure
            daily_returns = []
            v10_cash_days = 0
            v10_degross_days = 0
            for d in dates:
                active_rebals = [rd for rd in rebal_dates if rd <= d]
                if not active_rebals:
                    continue
                latest_rebal = active_rebals[-1]
                active_stocks = cohort_picks.get(latest_rebal, set())

                day_df = test_df[test_df['date'] == d]
                port_df = day_df[day_df['stock_code'].isin(active_stocks)]

                if len(port_df) == 0:
                    continue

                raw_ret = port_df[return_col].mean()

                # Daily gross exposure based on regime + VKOSPI
                exposure = 1.0
                regime_bad = MARKET_REGIME.get(d, 0) < args.regime_threshold if MARKET_REGIME else False
                vkospi_spike = _v10_fear_spike.get(d, False)

                if regime_bad and vkospi_spike:
                    exposure = 0.0
                    v10_cash_days += 1
                elif regime_bad:
                    exposure = 0.5
                    v10_degross_days += 1
                elif vkospi_spike:
                    exposure = 0.5
                    v10_degross_days += 1

                daily_returns.append(raw_ret * exposure)

            q5_raw = np.mean(daily_returns) * 100 if daily_returns else 0
            top_df = test_df[test_df['pred_rank'] <= TOP_N]

            # Track cash/degross for reporting
            total_days = len(daily_returns) if daily_returns else 1
            cash_ratio = v10_cash_days / total_days
            held_cash = (v10_cash_days + v10_degross_days) > 0

            # Bottom N
            max_rank = test_df.groupby('date')['pred_rank'].transform('max')
            bottom_df = test_df[test_df['pred_rank'] > max_rank - TOP_N]
            q1_raw = bottom_df[return_col].mean() * 100

            # 42d rebalance + buffer zone reduces turnover even more
            effective_slippage = ROUND_TRIP_COST * 0.4
            q5 = q5_raw - effective_slippage * 100
            q1 = q1_raw - effective_slippage * 100
            spread = q5 - q1 if not (np.isnan(q5) or np.isnan(q1)) else np.nan

        elif args.v9:
            # V9: Buffer zone portfolio â€” reduce turnover via hold threshold
            # Buy threshold: pred_rank <= TOP_N (50)
            # Hold threshold: pred_rank <= TOP_N * 2.4 (120) â€” existing holdings stay
            # Rebalance every 21 trading days
            # Regime/VKOSPI checks integrated at daily level (not annual post-hoc)
            test_df = test_df.sort_values(['date', 'pred_rank']).copy()
            test_df['sector'] = test_df['sector'].fillna('Unknown')
            dates = sorted(test_df['date'].unique())

            rebal_dates = dates[::21]
            hold_threshold = int(TOP_N * 2.4)

            # Pre-compute daily VKOSPI spike signal for this year
            _v9_fear_spike = {}
            if 'fear_index_level' in test_df.columns:
                fear_by_date = test_df.groupby('date')['fear_index_level'].first().sort_index()
                fear_ma20 = fear_by_date.rolling(20, min_periods=10).mean()
                for d in fear_by_date.index:
                    if pd.notna(fear_by_date[d]) and pd.notna(fear_ma20.get(d)):
                        _v9_fear_spike[d] = fear_by_date[d] > (fear_ma20[d] * 1.2)
                    else:
                        _v9_fear_spike[d] = False

            prev_portfolio = set()
            cohort_picks = {}
            turnover_list = []
            for rd in rebal_dates:
                day_df = test_df[test_df['date'] == rd].copy()
                day_df = day_df.sort_values('pred_rank')

                # New buys: rank <= TOP_N
                new_buys = set(day_df[day_df['pred_rank'] <= TOP_N]['stock_code'].tolist())

                # Existing holdings that survive hold threshold
                if prev_portfolio:
                    hold_df = day_df[day_df['stock_code'].isin(prev_portfolio)]
                    survivors = set(hold_df[hold_df['pred_rank'] <= hold_threshold]['stock_code'].tolist())
                else:
                    survivors = set()

                # Portfolio = new buys + survivors
                portfolio = new_buys | survivors
                cohort_picks[rd] = portfolio

                # Turnover tracking
                if prev_portfolio:
                    buys = portfolio - prev_portfolio
                    sells = prev_portfolio - portfolio
                    denom = max(len(portfolio), 1)
                    turnover_list.append((len(buys) + len(sells)) / (2 * denom))

                prev_portfolio = portfolio
            avg_turnover = np.mean(turnover_list) if turnover_list else 0

            # Compute daily returns with per-day regime/VKOSPI gross exposure
            daily_returns = []
            v9_cash_days = 0
            v9_degross_days = 0
            for d in dates:
                active_rebals = [rd for rd in rebal_dates if rd <= d]
                if not active_rebals:
                    continue
                latest_rebal = active_rebals[-1]
                active_stocks = cohort_picks.get(latest_rebal, set())

                day_df = test_df[test_df['date'] == d]
                port_df = day_df[day_df['stock_code'].isin(active_stocks)]

                if len(port_df) == 0:
                    continue

                raw_ret = port_df[return_col].mean()

                # Daily gross exposure based on regime + VKOSPI
                exposure = 1.0
                regime_bad = MARKET_REGIME.get(d, 0) < args.regime_threshold if MARKET_REGIME else False
                vkospi_spike = _v9_fear_spike.get(d, False)

                if regime_bad and vkospi_spike:
                    exposure = 0.0   # Both signals â†’ 100% cash
                    v9_cash_days += 1
                elif regime_bad:
                    exposure = 0.5   # Regime only â†’ 50% de-gross
                    v9_degross_days += 1
                elif vkospi_spike:
                    exposure = 0.5   # VKOSPI spike only â†’ 50% de-gross
                    v9_degross_days += 1

                daily_returns.append(raw_ret * exposure)

            q5_raw = np.mean(daily_returns) * 100 if daily_returns else 0
            top_df = test_df[test_df['pred_rank'] <= TOP_N]

            # Track cash/degross for reporting
            total_days = len(daily_returns) if daily_returns else 1
            cash_ratio = v9_cash_days / total_days
            held_cash = (v9_cash_days + v9_degross_days) > 0

            # Bottom N
            max_rank = test_df.groupby('date')['pred_rank'].transform('max')
            bottom_df = test_df[test_df['pred_rank'] > max_rank - TOP_N]
            q1_raw = bottom_df[return_col].mean() * 100

            # Buffer zone reduces turnover ~50%
            effective_slippage = ROUND_TRIP_COST * 0.5
            q5 = q5_raw - effective_slippage * 100
            q1 = q1_raw - effective_slippage * 100
            spread = q5 - q1 if not (np.isnan(q5) or np.isnan(q1)) else np.nan

        elif args.v8:
            # V8: Overlapping portfolios â€” 3 cohorts, each held 63 days, staggered 21 days
            test_df = test_df.sort_values(['date', 'pred_rank']).copy()
            test_df['sector'] = test_df['sector'].fillna('Unknown')
            dates = sorted(test_df['date'].unique())

            # Rebalance dates: every 21 trading days
            rebal_dates = dates[::21]

            # For each rebalance date, select top N stocks with sector constraint
            cohort_picks = {}
            for rd in rebal_dates:
                day_df = test_df[test_df['date'] == rd].copy()
                day_df = day_df.sort_values('pred_rank')
                day_df['_sc'] = day_df.groupby('sector').cumcount() + 1
                picks = day_df[day_df['_sc'] <= MAX_PER_SECTOR].head(TOP_N)
                cohort_picks[rd] = set(picks['stock_code'].tolist())

            # For each date, active portfolio = union of 3 most recent cohorts
            daily_returns = []
            for d in dates:
                active_rebals = [rd for rd in rebal_dates if rd <= d][-3:]
                active_stocks = set()
                for rd in active_rebals:
                    active_stocks |= cohort_picks.get(rd, set())

                day_df = test_df[test_df['date'] == d]
                port_df = day_df[day_df['stock_code'].isin(active_stocks)]

                if len(port_df) > 0:
                    daily_returns.append(port_df[return_col].mean())

            q5_raw = np.mean(daily_returns) * 100 if daily_returns else 0
            # Also compute top_df for downstream compatibility (last rebalance cohort)
            top_df = test_df[test_df['pred_rank'] <= TOP_N]

            # Bottom N
            max_rank = test_df.groupby('date')['pred_rank'].transform('max')
            bottom_df = test_df[test_df['pred_rank'] > max_rank - TOP_N]
            q1_raw = bottom_df[return_col].mean() * 100

            # Reduced slippage: only 1/3 portfolio turns over each rebalance
            effective_slippage = ROUND_TRIP_COST / 3
            q5 = q5_raw - effective_slippage * 100
            q1 = q1_raw - effective_slippage * 100
            spread = q5 - q1 if not (np.isnan(q5) or np.isnan(q1)) else np.nan

        elif QEPM_MODE:
            # QEPM: ì„¹í„° ì œì•½ + ë³€ë™ì„± ê°€ì¤‘ (ì™„ì „ ë²¡í„°í™”)
            test_df = test_df.sort_values(['date', 'pred_rank']).copy()
            test_df['sector'] = test_df['sector'].fillna('Unknown')

            # ì„¹í„°ë³„ ëˆ„ì  ì¹´ìš´íŠ¸ (ë‚ ì§œ+ì„¹í„° ê·¸ë£¹ë³„)
            test_df['_sector_cumcount'] = test_df.groupby(['date', 'sector']).cumcount() + 1
            test_df['_date_cumcount'] = test_df.groupby('date').cumcount() + 1

            # ì„¹í„° ì œì•½ + Top N í•„í„° (ì™„ì „ ë²¡í„°í™”)
            top_df = test_df[
                (test_df['_sector_cumcount'] <= MAX_PER_SECTOR) &
                (test_df['_date_cumcount'] <= TOP_N * 2)  # ì—¬ìœ ìˆê²Œ í•„í„°
            ].copy()

            # ë‚ ì§œë³„ë¡œ ë‹¤ì‹œ Top N ì„ íƒ
            top_df['_final_rank'] = top_df.groupby('date').cumcount() + 1
            top_df = top_df[top_df['_final_rank'] <= TOP_N]

            # ë³€ë™ì„± ì—­ê°€ì¤‘ ìˆ˜ìµë¥  (ë²¡í„°í™”)
            if 'volatility_20d' in top_df.columns:
                vol = top_df['volatility_20d'].fillna(top_df['volatility_20d'].median()).clip(lower=0.1)
                top_df['_inv_vol'] = 1 / vol
                top_df['_weight'] = top_df.groupby('date')['_inv_vol'].transform(lambda x: x / x.sum())
                q5_raw = (top_df[return_col] * top_df['_weight']).groupby(top_df['date']).sum().mean() * 100
            else:
                q5_raw = top_df[return_col].mean() * 100
        else:
            # ì¼ë°˜: ë‹¨ìˆœ Top N
            top_df = test_df[test_df['pred_rank'] <= TOP_N]
            q5_raw = top_df[return_col].mean() * 100

        if not args.v8 and not args.v9 and not args.v10:
            # Bottom N (ë²¡í„°í™”) â€” V8/V9/V10 compute this above
            max_rank = test_df.groupby('date')['pred_rank'].transform('max')
            bottom_df = test_df[test_df['pred_rank'] > max_rank - TOP_N]
            q1_raw = bottom_df[return_col].mean() * 100

            # QEPMì€ íšŒì „ìœ¨ ê°ì†Œë¡œ ìŠ¬ë¦¬í”¼ì§€ 1/3
            effective_slippage = ROUND_TRIP_COST / 3 if QEPM_MODE else ROUND_TRIP_COST
            q5 = q5_raw - effective_slippage * 100
            q1 = q1_raw - effective_slippage * 100
            spread = q5 - q1 if not (np.isnan(q5) or np.isnan(q1)) else np.nan
    else:
        # Quintile ìˆ˜ìµë¥ 
        test_df['quintile'] = pd.qcut(
            test_df['pred_rank'], 5,
            labels=[5,4,3,2,1], duplicates='drop'  # 1=top, 5=bottom
        )
        quintile_ret = test_df.groupby('quintile')[return_col].mean() * 100
        q1_raw = quintile_ret.get(1, np.nan)  # Bottom 20%
        q5_raw = quintile_ret.get(5, np.nan)  # Top 20%
        # ìŠ¬ë¦¬í”¼ì§€ ì ìš© (ì™•ë³µ ë¹„ìš© ì°¨ê°)
        q5 = q5_raw - ROUND_TRIP_COST * 100 if not np.isnan(q5_raw) else np.nan
        q1 = q1_raw - ROUND_TRIP_COST * 100 if not np.isnan(q1_raw) else np.nan
        spread = q5 - q1 if not (np.isnan(q5) or np.isnan(q1)) else np.nan

    # IC ê³„ì‚°
    ic, _ = stats.spearmanr(
        test_df['pred_score'].fillna(0),
        test_df[return_col].fillna(0)
    )

    # QEPM: ì‹œì¥(KOSPI200) ìˆ˜ìµë¥  - DBì—ì„œ ê°€ì ¸ì˜¤ê¸°
    if QEPM_MODE:
        market_ret = get_benchmark_return(test_year, HORIZON)
        abs_top = q5 + market_ret  # ì ˆëŒ€ ìˆ˜ìµë¥  = Alpha + ì‹œì¥
        abs_bot = q1 + market_ret
    else:
        market_ret = 0
        abs_top = q5
        abs_bot = q1

    # ì‹¤ì œ í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ìµë¥  ê³„ì‚° (Alpha ì•„ë‹Œ ì ˆëŒ€ ìˆ˜ìµë¥ !)
    abs_return_col = f'forward_return_{HORIZON}d'
    if (args.v8 or args.v9 or args.v10) and TOP_N > 0:
        # V8/V9/V10: portfolio_return comes from cohort-based daily returns
        portfolio_return = q5  # already computed from overlapping/buffer cohorts
    elif TOP_N > 0 and abs_return_col in test_df.columns:
        if QEPM_MODE:
            # QEPM: top_df ì¬ì‚¬ìš© (ì´ë¯¸ ì„¹í„° ì œì•½ ì ìš©ë¨)
            portfolio_return = top_df[abs_return_col].mean() * 100 - ROUND_TRIP_COST * 100
        else:
            # ì¼ë°˜: Top N í‰ê· 
            portfolio_return = top_df[abs_return_col].mean() * 100 - ROUND_TRIP_COST * 100
    else:
        portfolio_return = q5  # fallback

    # ğŸ›¡ï¸ í˜„ê¸ˆ íƒ€ì´ë°: ì‹œì¥ Regimeì´ ë‚˜ì˜ë©´ í˜„ê¸ˆ ë³´ìœ  (0% ìˆ˜ìµ)
    # V9/V10 handles regime/VKOSPI at daily level inside buffer zone portfolio loop
    if not args.v9 and not args.v10:
        held_cash = False
        cash_ratio = 0.0
    stop_loss_impact = 0.0

    if not args.v9 and not args.v10 and args.cash_timing and MARKET_REGIME:
        year_dates = [d for d in MARKET_REGIME.keys() if d.startswith(str(test_year))]
        if year_dates:
            regimes = [MARKET_REGIME[d] for d in year_dates if pd.notna(MARKET_REGIME[d])]
            if regimes:
                avg_regime = np.mean(regimes)

                if args.v42:
                    # ğŸ”¥ V4.2: ê³µê²©ì  Market Exit
                    # Regime < 0 (MA ì´í•˜)ì´ë©´ í•´ë‹¹ ê¸°ê°„ 100% í˜„ê¸ˆ
                    bad_days = sum(1 for r in regimes if r < args.regime_threshold)
                    cash_ratio = bad_days / len(regimes)

                    if cash_ratio > 0:
                        held_cash = True
                        # í˜„ê¸ˆ ë³´ìœ  ê¸°ê°„ì˜ ìˆ˜ìµ = 0 (ì†ì‹¤ íšŒí”¼)
                        # íˆ¬ì ê¸°ê°„ì˜ ìˆ˜ìµë§Œ ë°˜ì˜
                        portfolio_return = portfolio_return * (1 - cash_ratio)

                    # ğŸ”¥ V4.2: ì†ì ˆ ì‹œë®¬ë ˆì´ì…˜ (21ì¼ ë‚´ -7% í•˜ë½ ì‹œ)
                    # ì†ì ˆëœ ì¢…ëª©ì€ ìˆ˜ìµ ê¸°ì—¬ë„ê°€ ë‚®ì•„ì§
                    if abs_return_col in top_df.columns and 'drawdown_from_high' in top_df.columns:
                        # í° ë‚™í­ ì¢…ëª© ë¹„ìœ¨ ê³„ì‚° (ì†ì ˆ ì‹œë®¬ë ˆì´ì…˜)
                        severe_drawdown = top_df['drawdown_from_high'] < -args.stop_loss
                        stop_loss_ratio = severe_drawdown.mean() if len(top_df) > 0 else 0

                        # ì†ì ˆ ì¢…ëª©ì˜ ì†ì‹¤ ì œí•œ (-7%ì—ì„œ ì†ì ˆ)
                        if stop_loss_ratio > 0:
                            avg_loss_avoided = (top_df.loc[severe_drawdown, abs_return_col].mean() + args.stop_loss)
                            if pd.notna(avg_loss_avoided) and avg_loss_avoided < 0:
                                stop_loss_impact = -avg_loss_avoided * stop_loss_ratio * 100
                                portfolio_return += stop_loss_impact
                else:
                    # ê¸°ì¡´ ë°©ì‹: ë¶€ë¶„ í˜„ê¸ˆ ë¹„ì¤‘ ì¡°ì ˆ
                    bad_days = sum(1 for r in regimes if r < args.regime_threshold or r > 0.15)
                    cash_ratio = bad_days / len(regimes)

                    if cash_ratio > 0.2:
                        held_cash = True
                        portfolio_return = portfolio_return * (1 - min(cash_ratio, 0.8))

    # V8: VKOSPI spike de-grossing (replaces blunt 120d MA cash timing)
    # V9 handles VKOSPI de-grossing daily inside buffer zone loop; V8 uses post-hoc
    if args.v8 and 'fear_index_level' in df.columns:
        year_df = df[df['year'] == test_year]
        if len(year_df) > 0:
            fear_by_date = year_df.groupby('date')['fear_index_level'].first()
            fear_ma20 = fear_by_date.rolling(20, min_periods=10).mean()
            # Spike = current > 1.2x 20-day mean
            spike_mask = fear_by_date > (fear_ma20 * 1.2)
            spike_ratio = spike_mask.mean() if len(spike_mask) > 0 else 0
            if spike_ratio > 0:
                # Reduce position by 50% during spike days
                degross_factor = 1 - (spike_ratio * 0.5)
                portfolio_return = portfolio_return * degross_factor
                held_cash = True
                cash_ratio = max(cash_ratio, spike_ratio * 0.5)

    # ì—°ê°„ ì‹¤ì œ ìˆ˜ìµë¥  = ë¶„ê¸° ìˆ˜ìµë¥  ë³µë¦¬ (4íšŒ ë¦¬ë°¸ëŸ°ì‹± ê°€ì •)
    annual_portfolio = ((1 + portfolio_return/100) ** (252/HORIZON) - 1) * 100

    # Store test_df for decile/sector analysis
    all_test_dfs[test_year] = test_df

    all_results.append({
        'year': test_year,
        'Q1': q1,
        'Q5': q5,
        'spread': spread,
        'IC': ic,
        'market_ret': market_ret,
        'abs_top': abs_top,
        'portfolio_return': portfolio_return,  # ì‹¤ì œ ë¶„ê¸° ìˆ˜ìµë¥ 
        'annual_return': annual_portfolio,     # ì‹¤ì œ ì—°í™˜ì‚° ìˆ˜ìµë¥ 
        'train_samples': _n_train,
        'test_samples': _n_test,
        'held_cash': held_cash,
        'cash_ratio': cash_ratio,              # V4.2: í˜„ê¸ˆ ë³´ìœ  ë¹„ìœ¨
        'stop_loss_impact': stop_loss_impact,  # V4.2: ì†ì ˆ íš¨ê³¼
        'avg_turnover': avg_turnover           # Per-rebalance turnover
    })

    if not np.isnan(spread):
        bar = 'â–ˆ' * int(max(0, min(annual_portfolio, 50)))
        ic_status = 'âœ…' if ic > 0 else 'âŒ'
        if args.v42 and held_cash:
            cash_status = f'ğŸ’µ{cash_ratio*100:.0f}%'
        elif held_cash:
            cash_status = 'ğŸ’µ'
        else:
            cash_status = ''
        # ëª¨ë“  ëª¨ë“œì—ì„œ ì‹¤ì œ ì—°ìˆ˜ìµë¥  í‘œì‹œ
        print(f'  {test_year}: ë¶„ê¸°={portfolio_return:+5.1f}% | ì—°í™˜ì‚°={annual_portfolio:+5.1f}% | IC={ic:+.3f} {ic_status} {cash_status} {bar}')

print(f'  â± [2/3] Walk-Forward ë°±í…ŒìŠ¤íŠ¸: {time.time()-stage_t0:.1f}s')

# ============================================================================
# [3/3] Quant Tearsheet Report
# ============================================================================
stage_t0 = time.time()
print('\n[3/3] Quant Tearsheet')
print('=' * 70)

results_df = pd.DataFrame(all_results)

# --- Load KOSPI200 benchmark returns (annualized, same compounding) --------
benchmark_by_year = load_all_benchmark_returns(HORIZON)
# Annualize benchmark the same way as portfolio
bm_annual_by_year = {}
for yr, pct_ret in benchmark_by_year.items():
    bm_annual_by_year[yr] = ((1 + pct_ret / 100) ** (252 / HORIZON) - 1) * 100

# Add benchmark columns to results_df
results_df['benchmark_return'] = results_df['year'].map(bm_annual_by_year)
results_df['alpha'] = results_df['annual_return'] - results_df['benchmark_return']

# Filter to rows with valid data
valid = results_df.dropna(subset=['annual_return', 'benchmark_return'])
port_rets = valid['annual_return'].values
bm_rets = valid['benchmark_return'].values
alphas = valid['alpha'].values
ic_series = results_df['IC'].dropna().values

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Block 1: IC Analysis
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
mean_ic = np.mean(ic_series)
std_ic = np.std(ic_series, ddof=1) if len(ic_series) > 1 else 1e-9
icir = mean_ic / std_ic if std_ic > 1e-9 else 0.0
ic_hit_rate = np.mean(ic_series > 0) * 100
ic_strong_rate = np.mean(ic_series > 0.02) * 100
best_ic_idx = np.argmax(ic_series)
worst_ic_idx = np.argmin(ic_series)

print('\nIC Summary')
print('-' * 45)
print(f'  Mean IC          : {mean_ic:+.3f}')
print(f'  IC Std           :  {std_ic:.3f}')
print(f'  ICIR (IC/Std)    :  {icir:.2f}')
print(f'  IC Hit Rate      : {ic_hit_rate:.0f}% (>0)')
print(f'  IC > 0.02 Rate   : {ic_strong_rate:.0f}%')
print()
print(f'  {"Year":<6} {"IC":>8}  Note')
print(f'  {"----":<6} {"------":>8}  ----')
for i, (_, row) in enumerate(results_df.iterrows()):
    ic_val = row['IC']
    if pd.isna(ic_val):
        continue
    note = ''
    if i == best_ic_idx:
        note = '<- best'
    elif i == worst_ic_idx:
        note = '<- worst'
    print(f'  {int(row["year"]):<6} {ic_val:>+8.3f}  {note}')

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Block 1.5: Decile Analysis (pred_score â†’ forward return monotonicity)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if all_test_dfs:
    print('\nDecile Analysis (pred_score â†’ forward return)')
    print('-' * 70)
    decile_returns_all = {}  # decile -> list of returns across years
    for yr in sorted(all_test_dfs.keys()):
        tdf = all_test_dfs[yr]
        valid_mask = tdf['pred_score'].notna() & tdf[return_col].notna()
        tdf_valid = tdf[valid_mask]
        if len(tdf_valid) < 100:
            continue
        try:
            tdf_valid = tdf_valid.copy()
            tdf_valid['decile'] = pd.qcut(tdf_valid['pred_score'], 10, labels=False, duplicates='drop') + 1
            dec_ret = tdf_valid.groupby('decile')[return_col].mean() * 100
            for d in dec_ret.index:
                decile_returns_all.setdefault(d, []).append(dec_ret[d])
        except Exception:
            pass

    if decile_returns_all:
        n_deciles = max(decile_returns_all.keys())
        print(f'  {"Decile":<8}', end='')
        for d in range(1, n_deciles + 1):
            print(f' {"D"+str(d):>7}', end='')
        print()
        print(f'  {"------":<8}', end='')
        for _ in range(n_deciles):
            print(f' {"------":>7}', end='')
        print()
        avg_dec = {}
        for d in range(1, n_deciles + 1):
            vals = decile_returns_all.get(d, [0])
            avg_dec[d] = np.mean(vals)
        print(f'  {"Avg":>7}', end='')
        for d in range(1, n_deciles + 1):
            print(f' {avg_dec[d]:>+6.1f}%', end='')
        print()
        # Monotonicity check: D10 > D9 > ... > D1 (higher decile = higher pred_score = higher return)
        mono_ok = all(avg_dec.get(d+1, 0) >= avg_dec.get(d, 0) for d in range(1, n_deciles))
        mono_str = 'YES' if mono_ok else 'NO'
        spread_d = avg_dec.get(n_deciles, 0) - avg_dec.get(1, 0)
        print(f'\n  D{n_deciles}-D1 Spread: {spread_d:+.1f}%   Monotonic: {mono_str}')

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Block 1.6: IC Decay Analysis (signal strength at multiple horizons)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if all_test_dfs:
    print('\nIC Decay (Signal Strength by Horizon)')
    print('-' * 55)
    decay_horizons = [5, 10, 21, 42]
    horizon_ics = {h: [] for h in decay_horizons}

    for yr in sorted(all_test_dfs.keys()):
        tdf = all_test_dfs[yr].copy()
        if 'pred_score' not in tdf.columns:
            continue
        tdf = tdf.sort_values(['stock_code', 'date'])
        grouped = tdf.groupby('stock_code')
        for h in decay_horizons:
            col = f'_fwd_{h}d'
            if f'forward_return_{h}d' in tdf.columns:
                fwd = tdf[f'forward_return_{h}d']
            else:
                # Compute on-the-fly from closing_price
                if 'closing_price' in tdf.columns:
                    fwd = grouped['closing_price'].shift(-h) / tdf['closing_price'] - 1
                else:
                    continue
            valid_mask = tdf['pred_score'].notna() & fwd.notna()
            if valid_mask.sum() > 50:
                ic_h, _ = stats.spearmanr(tdf.loc[valid_mask, 'pred_score'], fwd[valid_mask])
                horizon_ics[h].append(ic_h)

    print(f'  {"Horizon":<10} {"Mean IC":>8} {"Std":>7} {"ICIR":>7} {"Decay%":>8}')
    print(f'  {"-------":<10} {"------":>8} {"---":>7} {"----":>7} {"------":>8}')
    base_ic = None
    for h in decay_horizons:
        ics = horizon_ics[h]
        if ics:
            m = np.mean(ics)
            s = np.std(ics, ddof=1) if len(ics) > 1 else 1e-9
            ir = m / s if s > 1e-9 else 0
            if base_ic is None:
                base_ic = m
                decay_pct = 0
            else:
                decay_pct = ((m - base_ic) / abs(base_ic) * 100) if abs(base_ic) > 1e-9 else 0
            print(f'  {h:>4}d     {m:>+7.3f}  {s:>6.3f}  {ir:>6.2f}  {decay_pct:>+6.0f}%')

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Block 2: Return Attribution (Portfolio vs Benchmark)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print(f'\nAnnual Returns: Portfolio vs KOSPI200  (Top {TOP_N}, slip {args.slippage}%)')
print('-' * 68)
print(f'  {"Year":<6} {"Portfolio":>10} {"KOSPI200":>10} {"Alpha":>10} {"Cash%":>7} {"Holdings":>9}')
print('-' * 68)
for _, row in results_df.iterrows():
    yr = int(row['year'])
    ann = row['annual_return']
    bm = row.get('benchmark_return', np.nan)
    alp = row.get('alpha', np.nan)
    cr = row['cash_ratio'] * 100
    if pd.isna(ann):
        continue
    bm_str = f'{bm:>+9.1f}%' if pd.notna(bm) else f'{"n/a":>10}'
    alp_str = f'{alp:>+9.1f}%' if pd.notna(alp) else f'{"n/a":>10}'
    print(f'  {yr:<6} {ann:>+9.1f}% {bm_str} {alp_str} {cr:>5.0f}%  {TOP_N:>7}')
print('-' * 68)
if len(valid) > 0:
    print(f'  {"Mean":<6} {np.mean(port_rets):>+9.1f}% {np.mean(bm_rets):>+9.1f}% {np.mean(alphas):>+9.1f}%')
    print(f'  {"Median":<6} {np.median(port_rets):>+9.1f}% {np.median(bm_rets):>+9.1f}% {np.median(alphas):>+9.1f}%')

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Block 3: Risk Metrics
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RF = 3.0  # Korea risk-free rate approx

if len(port_rets) > 1:
    port_mean = np.mean(port_rets)
    port_std = np.std(port_rets, ddof=1)
    bm_mean = np.mean(bm_rets) if len(bm_rets) > 0 else 0
    bm_std = np.std(bm_rets, ddof=1) if len(bm_rets) > 1 else 1e-9

    # Sharpe
    port_sharpe = (port_mean - RF) / port_std if port_std > 1e-9 else 0
    bm_sharpe = (bm_mean - RF) / bm_std if bm_std > 1e-9 else 0

    # Sortino (downside deviation: years below Rf)
    port_downside = port_rets[port_rets < RF] - RF
    port_dd_std = np.std(port_downside, ddof=1) if len(port_downside) > 1 else (abs(port_downside[0]) if len(port_downside) == 1 else 1e-9)
    port_sortino = (port_mean - RF) / port_dd_std if port_dd_std > 1e-9 else 0

    bm_downside = bm_rets[bm_rets < RF] - RF
    bm_dd_std = np.std(bm_downside, ddof=1) if len(bm_downside) > 1 else (abs(bm_downside[0]) if len(bm_downside) == 1 else 1e-9)
    bm_sortino = (bm_mean - RF) / bm_dd_std if bm_dd_std > 1e-9 else 0

    # Max Drawdown (worst single-year, proxy)
    port_maxdd = np.min(port_rets)
    bm_maxdd = np.min(bm_rets) if len(bm_rets) > 0 else 0

    # Calmar
    port_calmar = port_mean / abs(port_maxdd) if abs(port_maxdd) > 1e-9 else 0
    bm_calmar = bm_mean / abs(bm_maxdd) if abs(bm_maxdd) > 1e-9 else 0

    # Win rate
    port_winrate = np.mean(port_rets > 0) * 100
    bm_winrate = np.mean(bm_rets > 0) * 100 if len(bm_rets) > 0 else 0

    # Tracking error, Info Ratio, Beta, Alpha
    alpha_std = np.std(alphas, ddof=1) if len(alphas) > 1 else 1e-9
    tracking_error = alpha_std
    info_ratio = np.mean(alphas) / alpha_std if alpha_std > 1e-9 else 0

    # Beta = cov(port, bm) / var(bm)
    if len(port_rets) > 1 and len(bm_rets) > 1:
        cov_matrix = np.cov(port_rets, bm_rets)
        beta = cov_matrix[0, 1] / cov_matrix[1, 1] if cov_matrix[1, 1] > 1e-9 else 0
    else:
        beta = 0

    ann_alpha = np.mean(alphas)

    # MDD Duration: max consecutive losing years
    losing_streak = 0
    max_losing_streak = 0
    for r in port_rets:
        if r < 0:
            losing_streak += 1
            max_losing_streak = max(max_losing_streak, losing_streak)
        else:
            losing_streak = 0

    # VaR 95%
    port_var95 = np.percentile(port_rets, 5)
    bm_var95 = np.percentile(bm_rets, 5) if len(bm_rets) > 0 else 0

    # Annualized turnover from tracked data
    turnover_vals = results_df['avg_turnover'].values
    ann_turnover = np.mean(turnover_vals) * (252 / HORIZON) if np.any(turnover_vals > 0) else 0

    print('\nRisk-Adjusted Performance')
    print('-' * 50)
    print(f'  {"":22} {"Portfolio":>10} {"KOSPI200":>10}')
    print(f'  {"CAGR":<22} {port_mean:>+9.1f}% {bm_mean:>+9.1f}%')
    print(f'  {"Volatility":<22} {port_std:>9.1f}% {bm_std:>9.1f}%')
    print(f'  {"Sharpe (Rf=3%)":<22} {port_sharpe:>+9.2f}  {bm_sharpe:>+9.2f}')
    print(f'  {"Sortino":<22} {port_sortino:>+9.2f}  {bm_sortino:>+9.2f}')
    print(f'  {"Max Drawdown":<22} {port_maxdd:>+9.1f}% {bm_maxdd:>+9.1f}%')
    print(f'  {"MDD Duration (yrs)":<22} {max_losing_streak:>9}')
    print(f'  {"VaR 95%":<22} {port_var95:>+9.1f}% {bm_var95:>+9.1f}%')
    print(f'  {"Calmar Ratio":<22} {port_calmar:>+9.2f}  {bm_calmar:>+9.2f}')
    print(f'  {"Win Rate":<22} {port_winrate:>8.0f}%  {bm_winrate:>8.0f}%')
    print(f'  {"Best Year":<22} {np.max(port_rets):>+9.1f}% {np.max(bm_rets):>+9.1f}%')
    print(f'  {"Worst Year":<22} {port_maxdd:>+9.1f}% {bm_maxdd:>+9.1f}%')
    if ann_turnover > 0:
        print(f'  {"Ann. Turnover":<22} {ann_turnover*100:>8.0f}%')
    print('-' * 50)
    print(f'  {"Tracking Error":<22} {tracking_error:>9.1f}%')
    print(f'  {"Information Ratio":<22} {info_ratio:>+9.2f}')
    print(f'  {"Alpha (ann.)":<22} {ann_alpha:>+9.1f}%')
    print(f'  {"Beta vs KOSPI200":<22} {beta:>9.2f}')

    # Cumulative return tracking
    print('\nCumulative Returns')
    print('-' * 55)
    cum_port = 1.0
    cum_bm = 1.0
    print(f'  {"Year":<6} {"Port Ann":>10} {"Cum Port":>10} {"BM Ann":>10} {"Cum BM":>10}')
    print(f'  {"----":<6} {"-------":>10} {"-------":>10} {"------":>10} {"------":>10}')
    for _, row in valid.iterrows():
        yr = int(row['year'])
        p_ret = row['annual_return']
        b_ret = row['benchmark_return']
        cum_port *= (1 + p_ret / 100)
        cum_bm *= (1 + b_ret / 100)
        print(f'  {yr:<6} {p_ret:>+9.1f}% {(cum_port-1)*100:>+9.1f}% {b_ret:>+9.1f}% {(cum_bm-1)*100:>+9.1f}%')
    print(f'  {"Total":<6} {"":>10} {(cum_port-1)*100:>+9.1f}% {"":>10} {(cum_bm-1)*100:>+9.1f}%')
else:
    port_mean = np.mean(port_rets) if len(port_rets) > 0 else 0
    port_std = 0
    port_sharpe = 0
    port_maxdd = np.min(port_rets) if len(port_rets) > 0 else 0
    port_winrate = 0
    ann_alpha = np.mean(alphas) if len(alphas) > 0 else 0
    print('\nRisk-Adjusted Performance')
    print('-' * 50)
    print('  (Insufficient data for full risk analysis)')

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Block 4: Regime Analysis
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print('\nRegime Effectiveness')
print('-' * 55)
print(f'  {"Year":<6} {"Regime":>8} {"Cash%":>7} {"StopLoss":>10} {"Return":>10}')
print(f'  {"----":<6} {"------":>8} {"-----":>7} {"--------":>10} {"------":>10}')
for _, row in results_df.iterrows():
    yr = int(row['year'])
    ann = row['annual_return']
    if pd.isna(ann):
        continue
    cr = row['cash_ratio'] * 100
    sl = row.get('stop_loss_impact', 0)
    regime_str = f'{cr:>6.0f}%' if cr > 0 else f'{"0%":>7}'
    sl_str = f'{sl:>+9.1f}%' if sl != 0 else f'{"--":>10}'
    print(f'  {yr:<6} {"active" if cr > 0 else "off":>8} {regime_str} {sl_str} {ann:>+9.1f}%')
avg_cash = results_df['cash_ratio'].mean() * 100
cash_active_years = (results_df['cash_ratio'] > 0).sum()
print(f'\n  Avg cash ratio: {avg_cash:.1f}%   Active years: {cash_active_years}/{len(results_df)}')

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Block 5: Feature Importance (last model)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
importance = model.feature_importance()

print('\nFeature Importance (Top 15)')
print('-' * 55)

for i, row in importance.head(15).iterrows():
    feature = row['feature']
    imp = row['importance']

    # Group classification
    v8_new = ['inventory_sales_gap', 'current_ratio', 'hvup_ratio']
    if args.v9 and feature == 'short_term_reversal':
        group = 'V9'
    elif args.v8 and feature in v8_new:
        group = 'V8'
    elif (args.v8 or args.v6) and feature in fe.QUALITY_FEATURES:
        group = 'Qual'
    elif (args.v8 or args.v6) and feature in fe.V5_FEATURES:
        group = 'V5'
    elif args.v5:
        group = 'V5'
    elif feature in momentum_features:
        group = 'Mom'
    elif feature in volume_features:
        group = 'Vol'
    elif feature in volatility_features:
        group = 'Risk'
    elif feature in intuition_features:
        group = 'Intu'
    elif feature in traditional_features:
        group = 'Trad'
    elif args.v4 and feature in macro_features:
        group = 'Macr'
    else:
        group = 'Fund'

    bar = '|' * int(imp / importance['importance'].max() * 20)
    print(f'  {feature:<25} [{group:<4}] {bar}')

# Group importance
print('\nFeature Group Importance')
print('-' * 45)

if args.v9:
    v6_no_hvup = [c for c in fe.V6_FEATURES if c != 'hvup_ratio' and c in df.columns]
    v5_in_use = [c for c in fe.V5_FEATURES if c in df.columns]
    quality_in_use = [c for c in fe.QUALITY_FEATURES if c in df.columns]
    v9_new = [c for c in ['short_term_reversal'] if c in df.columns]
    groups = {'V5 Base': v5_in_use, 'Quality': quality_in_use, 'V9 New': v9_new}
elif args.v8:
    v5_in_use = [c for c in fe.V5_FEATURES if c in df.columns]
    quality_in_use = [c for c in fe.QUALITY_FEATURES if c in df.columns]
    v8_new_in_use = [c for c in ['inventory_sales_gap', 'current_ratio', 'hvup_ratio'] if c in df.columns]
    groups = {'V5 Base': v5_in_use, 'Quality': quality_in_use, 'V8 New': v8_new_in_use}
elif args.v6:
    v5_in_use = [c for c in fe.V5_FEATURES if c in df.columns]
    quality_in_use = [c for c in fe.QUALITY_FEATURES if c in df.columns]
    groups = {'V5 Base': v5_in_use, 'Quality': quality_in_use}
elif args.v5:
    groups = {'V5': all_features}
else:
    groups = {
        'Momentum': momentum_features,
        'Volume': volume_features,
        'Volatility': volatility_features,
        'Intuition': intuition_features,
        'Traditional': traditional_features,
        'Fundament.': fund_features,
    }
    if args.v4:
        groups['Macro'] = macro_features

total_imp = importance['importance'].sum()
for group_name, group_features in groups.items():
    group_imp = importance[importance['feature'].isin(group_features)]['importance'].sum()
    pct = group_imp / total_imp * 100
    bar = '|' * int(pct / 5)
    print(f'  {group_name:<12} {pct:>5.1f}% {bar}')

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Block 5.5: Sector Concentration Analysis
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if all_test_dfs and TOP_N > 0:
    print('\nSector Concentration (Top-N Portfolio)')
    print('-' * 55)
    sector_counts_all = {}  # sector -> total count across years
    sector_warnings = []
    for yr in sorted(all_test_dfs.keys()):
        tdf = all_test_dfs[yr]
        if 'sector' not in tdf.columns:
            continue
        top = tdf[tdf['pred_rank'] <= TOP_N].copy()
        if len(top) == 0:
            continue
        # Per-date sector counts, then average across dates
        dates_in_yr = top['date'].unique()
        yr_sector = {}
        for d in dates_in_yr:
            day_top = top[top['date'] == d]
            for s in day_top['sector'].fillna('Unknown'):
                yr_sector[s] = yr_sector.get(s, 0) + 1
        # Normalize by number of dates
        for s in yr_sector:
            avg_count = yr_sector[s] / len(dates_in_yr)
            sector_counts_all[s] = sector_counts_all.get(s, 0) + avg_count
        # Check concentration per year
        total_holdings = sum(yr_sector.values())
        for s, c in yr_sector.items():
            pct = c / total_holdings * 100
            if pct > 30:
                sector_warnings.append((yr, s, pct))

    # Show top 5 sectors by frequency
    top_sectors = sorted(sector_counts_all.items(), key=lambda x: -x[1])[:5]
    n_years = len(all_test_dfs)
    print(f'  {"Sector":<25} {"Avg Holdings/Year":>18}')
    print(f'  {"------":<25} {"----------------":>18}')
    for s, c in top_sectors:
        print(f'  {s:<25} {c/n_years:>17.1f}')

    if sector_warnings:
        print(f'\n  âš ï¸ Concentration Warnings (sector > 30% of portfolio):')
        for yr, s, pct in sector_warnings:
            print(f'    {yr}: {s} = {pct:.0f}%')
    else:
        print(f'\n  âœ… No sector exceeds 30% concentration')

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Block 6: Diagnostics / Verdict
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print('\nStrategy Diagnostics')
print('-' * 50)

checks = []

# 1. ICIR > 0.5
icir_pass = icir > 0.5
checks.append(('ICIR > 0.5', icir_pass, f'{icir:.2f}'))

# 2. IC hit rate > 60%
ic_hit_pass = ic_hit_rate > 60
checks.append(('IC hit rate > 60%', ic_hit_pass, f'{ic_hit_rate:.0f}%'))

# 3. Alpha > 0
alpha_pass = ann_alpha > 0
checks.append(('Alpha > 0', alpha_pass, f'{ann_alpha:+.1f}%'))

# 4. Sharpe > 0.3
sharpe_val = port_sharpe if len(port_rets) > 1 else 0
sharpe_pass = sharpe_val > 0.3
checks.append(('Sharpe > 0.3', sharpe_pass, f'{sharpe_val:+.2f}'))

# 5. MaxDD > -25%
maxdd_val = port_maxdd if len(port_rets) > 0 else 0
maxdd_pass = maxdd_val > -25
checks.append(('MaxDD > -25%', maxdd_pass, f'{maxdd_val:+.1f}%'))

# 6. Win rate > 50%
winrate_val = port_winrate if len(port_rets) > 1 else 0
winrate_pass = winrate_val > 50
checks.append(('Win rate > 50%', winrate_pass, f'{winrate_val:.0f}%'))

passed = 0
for label, ok, val in checks:
    tag = '[PASS]' if ok else '[FAIL]'
    print(f'  {tag} {label:<22}: {val}')
    if ok:
        passed += 1

total_checks = len(checks)
print('-' * 50)

# Verdict
ic_ok = icir_pass and ic_hit_pass
alpha_ok = alpha_pass and sharpe_pass
if passed == total_checks:
    verdict = 'All checks passed. Strategy is robust.'
elif ic_ok and not alpha_ok:
    verdict = f'Score: {passed}/{total_checks} -- Model has signal (IC), but portfolio construction destroys alpha.'
elif alpha_ok and not ic_ok:
    verdict = f'Score: {passed}/{total_checks} -- Returns OK, but IC quality is weak (may be lucky).'
elif passed >= total_checks * 0.5:
    verdict = f'Score: {passed}/{total_checks} -- Mixed results. Further investigation needed.'
else:
    verdict = f'Score: {passed}/{total_checks} -- Strategy needs significant rework.'
print(f'  {verdict}')

# Save results
results_df.to_csv('backtest_v2_results.csv', index=False)
print(f'\nResults saved: backtest_v2_results.csv')

print(f'\n  [3/3] Report: {time.time()-stage_t0:.1f}s')
print('=' * 70)
print('Backtest complete.')
print('=' * 70)
