"""
Feature Engineering V4 - ë§¤í¬ë¡œ Regime Detection í†µí•©

V2 â†’ V3 â†’ V4 ì§„í™”:
1. ëª¨ë©˜í…€/ìˆ˜ê¸‰ í”¼ì²˜ ëŒ€í­ ê°•í™” (ì¬ë¬´ 30-40% ëª©í‘œ)
2. ì„¹í„° ì¤‘ë¦½í™” (Sector Neutralization)
3. Delta í”¼ì²˜ ì¶”ê°€ (QoQ, YoY ë³€í™”)
4. ë³¸ëŠ¥ ì „ëµ í”¼ì²˜ (ë‚™í­ê³¼ëŒ€, ê±°ë˜ëŸ‰ í­ë°œ, ê³¼ê±° ì˜ê´‘)
5. [V4 ì‹ ê·œ] ë§¤í¬ë¡œ Regime Detection - 2021~2022 í­ë½ì¥ íšŒí”¼ìš©
   - market_regime_score: ì‹œì¥ ì´ê²©ë„
   - fear_index_delta: VKOSPI ë³€í™”
   - dollar_impact: ë‹¬ëŸ¬ì„ ë¬¼ ëª¨ë©˜í…€
   - bond_stock_spread: ì±„ê¶Œ vs ì£¼ì‹
   - sector_relative_momentum: ì„¹í„° ëŒ€ë¹„ ì¢…ëª© ì•ŒíŒŒ
"""

import gc
import pandas as pd
import numpy as np
import sqlite3
import logging
from typing import List, Optional
from pathlib import Path

# Import financial feature generator
import sys
sys.path.insert(0, str(Path(__file__).parent.parent))
from features.financial_features import FinancialFeatureGenerator
from ml.macro_features import MacroFeatureEngineer


class FeatureEngineer:
    """V3: í€€íŠ¸ íŒ€ì¥ í”¼ë“œë°± ë°˜ì˜ - í”¼ì²˜ ë‹¤ì´ì–´íŠ¸ + ë³¸ëŠ¥ ê°•í™”"""

    # =========================================================================
    # í”¼ì²˜ ê·¸ë£¹ ì •ì˜ (V3: 45ê°œ â†’ 25ê°œë¡œ ì••ì¶•)
    # =========================================================================

    # [ê·¸ë£¹ 1] ëª¨ë©˜í…€ í”¼ì²˜ - V4.1 í„°ë³´ ì—”ì§„ ğŸ”¥
    MOMENTUM_FEATURES = [
        'mom_5d',                # ë‹¨ê¸° ëª¨ë©˜í…€ (1ì£¼)
        'mom_60d',               # ì¤‘ê¸° ëª¨ë©˜í…€ (3ê°œì›”)
        'mom_126d',              # ì¥ê¸° ëª¨ë©˜í…€ (6ê°œì›”)
        'rs_vs_market_20d',      # ì‹œì¥ ëŒ€ë¹„ ìƒëŒ€ê°•ë„
        # === V4.1 ì‹ ê·œ ===
        'rs_vs_sector_20d',      # ğŸ”¥ ì„¹í„° ëŒ€ë¹„ ìƒëŒ€ê°•ë„ (ì„¹í„° ë‚´ ëŒ€ì¥ì£¼)
        'rs_acceleration',       # ğŸ”¥ ìƒëŒ€ê°•ë„ ê°€ì†ë„ (20d - 60d)
    ]

    # [ê·¸ë£¹ 2] ìˆ˜ê¸‰/ê±°ë˜ëŸ‰ í”¼ì²˜ - V4.1 í„°ë³´ ì—”ì§„ ğŸ”¥
    VOLUME_FEATURES = [
        'volume_surprise',       # ê±°ë˜ëŸ‰ í­ë°œ (20ì¼ í‰ê·  ëŒ€ë¹„)
        'volume_trend',          # ê±°ë˜ëŸ‰ ì¶”ì„¸ (5ì¼ vs 20ì¼)
        'value_surprise',        # ê±°ë˜ëŒ€ê¸ˆ í­ë°œ
        'accumulation_index',    # ëˆ„ì /ë°°ë¶„ ì§€í‘œ
        'smart_money_flow',      # ìŠ¤ë§ˆíŠ¸ë¨¸ë‹ˆ íë¦„
        'volume_breakout',       # ê±°ë˜ëŸ‰ ëŒíŒŒ ì‹ í˜¸
        # === V4.1 ì‹ ê·œ: "ëˆ ëƒ„ìƒˆ" í”¼ì²˜ ===
        'price_volume_synergy',  # ğŸ”¥ ê°€ê²©Ã—ê±°ë˜ëŸ‰ ì‹œë„ˆì§€ (ì§„ì§œ ìƒìŠ¹)
    ]

    # [ê·¸ë£¹ 3] ë³€ë™ì„±/ë¦¬ìŠ¤í¬ í”¼ì²˜ - V4.3 Beta ì¶”ê°€
    VOLATILITY_FEATURES = [
        'volatility_20d',
        'volatility_ratio',      # ë‹¨ê¸°/ì¥ê¸° ë³€ë™ì„± ë¹„ìœ¨ (VCP)
        'drawdown_from_high',    # ê³ ì  ëŒ€ë¹„ ë‚™í­ ğŸ”¥
        'recovery_from_low',     # ì €ì  ëŒ€ë¹„ ë°˜ë“±
        'rolling_beta',          # ğŸ”¥ V4.3: ì‹œì¥ Beta (Residual ê³„ì‚°ìš©)
    ]

    # [ê·¸ë£¹ 4] ë³¸ëŠ¥ ì „ëµ í”¼ì²˜ - V4.1 í„°ë³´ ì—”ì§„ ğŸ”¥
    INTUITION_FEATURES = [
        'past_glory_1y',         # 1ë…„ê°„ ìµœëŒ€ ìƒìŠ¹ë¥ 
        'fallen_angel_score',    # ì¶”ë½í•œ ì²œì‚¬ ì ìˆ˜
        'vcp_score',             # Volatility Contraction Pattern
        'glory_correction_volume',  # ì˜ê´‘ * ë‚™í­ * ê±°ë˜ëŸ‰í­ë°œ
        'fear_greed_signal',        # ê³µí¬ ì† íƒìš• ì‹ í˜¸
        'smart_accumulation',       # ìŠ¤ë§ˆíŠ¸ë¨¸ë‹ˆ ë§¤ì§‘ ì‹ í˜¸
        # === V4.1 ì‹ ê·œ: í­ë°œ ì§ì „ í¬ì°© ===
        'vcp_breakout_potential',   # ğŸ”¥ VCP Ã— ê±°ë˜ëŸ‰ì¶”ì„¸ (í­ë°œ ì„ë°•)
    ]

    # [ê·¸ë£¹ 5] ì „í†µì  ê¸°ìˆ  ì§€í‘œ - ì¶•ì†Œ
    TRADITIONAL_FEATURES = [
        'rsi_14',
        'bb_squeeze',            # ë³¼ë¦°ì €ë°´ë“œ ìˆ˜ì¶•
    ]

    # [ê·¸ë£¹ 6] ì¬ë¬´ í”¼ì²˜ - V4.1: ë‹¨ì¼ í”¼ì²˜ë¡œ ì••ì¶•! ğŸ”¥
    # "ì¬ë¬´ëŠ” ì…ì¥ê¶Œ, ìš°ìŠ¹ìëŠ” ëª¨ë©˜í…€ì—ì„œ ë‚˜ì˜¨ë‹¤"
    FUNDAMENTAL_FEATURES = [
        'financial_quality_index',  # ğŸ”¥ ì¬ë¬´ 4ê°œ í†µí•© (ROE + ë§¤ì¶œê°€ì† + ë§ˆì§„ê°œì„ )
    ]

    # ë‚´ë¶€ ê³„ì‚°ìš© (í”¼ì²˜ë¡œ ì§ì ‘ ì‚¬ìš© ì•ˆ í•¨)
    _FUNDAMENTAL_RAW = [
        'roe_delta_qoq',
        'roe_sector_zscore',
        'revenue_growth_accel',
        'margin_improvement',
    ]

    # [ê·¸ë£¹ 7] V4 ì‹ ê·œ: ë§¤í¬ë¡œ Regime Detection í”¼ì²˜ ğŸ”¥ğŸ”¥ğŸ”¥
    # "2021~2022ë…„ í­ë½ì¥ì„ í”¼í•˜ê¸° ìœ„í•œ ì‹œì¥ ì˜¨ë„ê³„"
    MACRO_FEATURES = [
        # 1ë‹¨ê³„: ì‹œì¥ì˜ ì˜¨ë„ê³„ (Regime Detection)
        'market_regime_score',      # KOSPI 200 ì´ê²©ë„ (120ì¼ MA ëŒ€ë¹„)
        'kosdaq_regime_score',      # KOSDAQ 150 ì´ê²©ë„
        'size_spread',              # ëŒ€í˜•ì£¼ - ì†Œí˜•ì£¼ ìˆ˜ìµë¥  (ìŒìˆ˜ = ë¶ˆì¥)
        'market_breadth',           # MA ìœ„ ì„¹í„° ë¹„ìœ¨ (0~1)

        # 3ë‹¨ê³„: ë§¤í¬ë¡œ ê³µí¬ ë ˆì´ë” (Inter-market Analysis)
        'fear_index_delta',         # VKOSPI 5ì¼ ë³€í™” (ê¸‰ë“± = ìœ„í—˜)
        'fear_index_level',         # VKOSPI ì ˆëŒ€ ë ˆë²¨
        'dollar_impact',            # ë‹¬ëŸ¬ì„ ë¬¼ 20ì¼ ëª¨ë©˜í…€ (ê¸‰ë“± = ì™¸ì¸ ì´íƒˆ)
        'bond_stock_spread',        # ì±„ê¶Œ - ì£¼ì‹ ìˆ˜ìµë¥  (ì–‘ìˆ˜ = Risk-off)

        # ë³µí•© í”¼ì²˜
        'macro_risk_score',         # ì¢…í•© ë§¤í¬ë¡œ ë¦¬ìŠ¤í¬ ì ìˆ˜
        'regime_momentum_interaction',  # regime * momentum ìƒí˜¸ì‘ìš©
    ]

    # [V5] Research-backed 10-Feature Mode
    V5_FEATURES = [
        'gp_over_assets', 'roe_delta_yoy', 'pb_sector_zscore',
        'intermediate_momentum', 'drawdown_from_high', 'fallen_angel_score',
        'volume_surprise', 'mom_5d', 'market_regime_score', 'rolling_beta_60d',
    ]

    # ì „ì²´ í”¼ì²˜ ë¦¬ìŠ¤íŠ¸
    FEATURE_COLUMNS = (
        MOMENTUM_FEATURES +
        VOLUME_FEATURES +
        VOLATILITY_FEATURES +
        INTUITION_FEATURES +
        TRADITIONAL_FEATURES +
        FUNDAMENTAL_FEATURES +
        MACRO_FEATURES  # V4 ì¶”ê°€
    )

    def __init__(self, db_path: str = "krx_stock_data.db"):
        self.db_path = db_path
        self.logger = logging.getLogger(__name__)

    def load_raw_data(self, start_date: str, end_date: str,
                      markets: List[str] = None) -> pd.DataFrame:
        """Load raw OHLCV data from database."""
        markets = markets or ['kospi', 'kosdaq']
        market_placeholders = ','.join(['?' for _ in markets])

        query = f"""
        SELECT
            dp.stock_code,
            dp.date,
            dp.market_type,
            dp.opening_price,
            dp.high_price,
            dp.low_price,
            dp.closing_price,
            dp.volume,
            dp.value,
            dp.market_cap,
            s.current_name as name,
            s.current_sector_type as sector
        FROM daily_prices dp
        JOIN stocks s ON dp.stock_code = s.stock_code
        WHERE dp.date >= ? AND dp.date <= ?
          AND dp.market_type IN ({market_placeholders})
          AND dp.closing_price > 0
          AND dp.volume > 0
        ORDER BY dp.stock_code, dp.date
        """

        params = [start_date, end_date] + markets

        conn = sqlite3.connect(self.db_path)
        df = pd.read_sql_query(query, conn, params=params)
        conn.close()

        self.logger.info(f"Loaded {len(df):,} rows from {start_date} to {end_date}")
        return df

    def compute_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Compute all V2 features."""
        self.logger.info("Computing V2 features (momentum-heavy)...")

        df = df.sort_values(['stock_code', 'date']).copy()
        grouped = df.groupby('stock_code')
        
        # Calculate returns first (needed for forward returns calculation)
        df['return'] = grouped['closing_price'].pct_change()
        df['log_return'] = np.log1p(df['return'])

        # ================================================================
        # [ê·¸ë£¹ 1] ëª¨ë©˜í…€ í”¼ì²˜
        # ================================================================
        self._compute_momentum_features(df, grouped)

        # ================================================================
        # [ê·¸ë£¹ 2] ìˆ˜ê¸‰/ê±°ë˜ëŸ‰ í”¼ì²˜
        # ================================================================
        self._compute_volume_features(df, grouped)

        # ================================================================
        # [ê·¸ë£¹ 3] ë³€ë™ì„±/ë¦¬ìŠ¤í¬ í”¼ì²˜
        # ================================================================
        self._compute_volatility_features(df, grouped)

        # ================================================================
        # [ê·¸ë£¹ 4] ë³¸ëŠ¥ ì „ëµ í”¼ì²˜
        # ================================================================
        self._compute_intuition_features(df, grouped)

        # ================================================================
        # [ê·¸ë£¹ 5] ì „í†µì  ê¸°ìˆ  ì§€í‘œ
        # ================================================================
        self._compute_traditional_features(df, grouped)

        # ================================================================
        # ì„¹í„° ì¤‘ë¦½í™” (Sector Neutralization)
        # ================================================================
        self._apply_sector_neutralization(df)

        # Cleanup
        self._cleanup_intermediate_cols(df)

        self.logger.info(f"Computed {len(self.FEATURE_COLUMNS)} V2 features")
        return df

    def _compute_momentum_features(self, df: pd.DataFrame, grouped) -> None:
        """ëª¨ë©˜í…€ í”¼ì²˜ ê³„ì‚° (V3: ì••ì¶•ë¨ - 5d, 60d, 126d, RSë§Œ) - ìµœì í™” ë²„ì „"""

        # Multi-timeframe momentum - ë²¡í„°í™” (pct_changeëŠ” ì´ë¯¸ ë¹ ë¦„)
        for period in [5, 20, 60, 120, 126]:
            df[f'mom_{period}d'] = grouped['closing_price'].pct_change(period)

        # V5: Intermediate Momentum (skip last month) - price_t-21 / price_t-126 - 1
        df['intermediate_momentum'] = grouped['closing_price'].shift(21) / grouped['closing_price'].shift(126) - 1

        # Moving averages - ë²¡í„°í™”ëœ rolling
        for period in [5, 20, 60, 120]:
            df[f'ma_{period}'] = grouped['closing_price'].rolling(
                period, min_periods=period//2
            ).mean().reset_index(level=0, drop=True)

        # dist_ma í•œ ë²ˆì— ê³„ì‚°
        for period in [5, 20, 60, 120]:
            df[f'dist_ma_{period}'] = (
                df['closing_price'] / df[f'ma_{period}'].clip(lower=1) - 1
            )

        # MA Trend (ì •ë°°ì—´ ì—¬ë¶€): 5 > 20 > 60
        df['ma_trend'] = (
            (df['ma_5'] > df['ma_20']).astype(float) * 0.5 +
            (df['ma_20'] > df['ma_60']).astype(float) * 0.5
        )

        # Relative Strength vs Market - ë²¡í„°í™”
        for period in [20, 60]:
            market_ret = df.groupby('date')[f'mom_{period}d'].transform('median')
            df[f'rs_vs_market_{period}d'] = df[f'mom_{period}d'] - market_ret

        # Momentum Consistency (ìƒìŠ¹ì¼ ë¹„ìœ¨)
        df['up_day'] = (df['return'] > 0).astype(float)
        df['mom_consistency'] = grouped['up_day'].rolling(
            20, min_periods=10
        ).mean().reset_index(level=0, drop=True)

        # Momentum Acceleration (ìµœê·¼ ëª¨ë©˜í…€ / ê³¼ê±° ëª¨ë©˜í…€)
        df['mom_acceleration'] = df['mom_20d'] / df['mom_60d'].clip(lower=0.01).abs()
        df['mom_acceleration'] = df['mom_acceleration'].clip(-5, 5)

        # === V4.1 ì‹ ê·œ: í„°ë³´ ëª¨ë©˜í…€ í”¼ì²˜ ===
        # RS Acceleration (ìƒëŒ€ê°•ë„ ê°€ì†ë„) - "ìµœê·¼ ë” ê°•í•´ì§€ëŠ” ë†ˆ"
        df['rs_acceleration'] = df['rs_vs_market_20d'] - df.get('rs_vs_market_60d', 0)
        df['rs_acceleration'] = df['rs_acceleration'].clip(-0.5, 0.5)

    def _compute_volume_features(self, df: pd.DataFrame, grouped) -> None:
        """ìˆ˜ê¸‰/ê±°ë˜ëŸ‰ í”¼ì²˜ ê³„ì‚° - ìµœì í™” ë²„ì „"""

        # Rolling ì—°ì‚°ì„ í•œ ë²ˆì— ë¬¶ì–´ì„œ ì²˜ë¦¬
        vol_rolling_5 = grouped['volume'].rolling(5, min_periods=3)
        vol_rolling_20 = grouped['volume'].rolling(20, min_periods=10)
        vol_rolling_60 = grouped['volume'].rolling(60, min_periods=30)

        df['vol_5d'] = vol_rolling_5.mean().reset_index(level=0, drop=True)
        df['vol_20d'] = vol_rolling_20.mean().reset_index(level=0, drop=True)
        df['vol_60d_max'] = vol_rolling_60.max().reset_index(level=0, drop=True)

        # Volume Surprise / Trend / Breakout - ë²¡í„° ì—°ì‚°
        df['volume_surprise'] = df['volume'] / df['vol_20d'].clip(lower=1)
        df['volume_trend'] = df['vol_5d'] / df['vol_20d'].clip(lower=1)
        df['volume_breakout'] = df['volume'] / df['vol_60d_max'].clip(lower=1)

        # Value Surprise (ê±°ë˜ëŒ€ê¸ˆ í­ë°œ)
        df['value_20d'] = grouped['value'].rolling(20, min_periods=10).mean().reset_index(level=0, drop=True)
        df['value_surprise'] = df['value'] / df['value_20d'].clip(lower=1)

        # Smart Money Flow (ì¢…ê°€ ìœ„ì¹˜ * ê±°ë˜ëŸ‰)
        df['close_location'] = (
            (df['closing_price'] - df['low_price']) /
            (df['high_price'] - df['low_price']).clip(lower=1)
        )
        df['daily_mf'] = (df['close_location'] * 2 - 1) * df['volume']

        # Rolling sums - í•œ ë²ˆì— ê³„ì‚°
        mf_rolling_sum = grouped['daily_mf'].rolling(20, min_periods=10).sum().reset_index(level=0, drop=True)
        vol_rolling_sum = grouped['volume'].rolling(20, min_periods=10).sum().reset_index(level=0, drop=True)
        df['smart_money_flow'] = mf_rolling_sum / vol_rolling_sum.clip(lower=1)

        # Accumulation Index
        df['accumulation_index'] = grouped['smart_money_flow'].rolling(
            10, min_periods=5
        ).mean().reset_index(level=0, drop=True)

        # === V4.1 ì‹ ê·œ: "ëˆ ëƒ„ìƒˆ" í”¼ì²˜ ===
        # Price-Volume Synergy (ê°€ê²©Ã—ê±°ë˜ëŸ‰ ì‹œë„ˆì§€) - "ì§„ì§œ ìƒìŠ¹"ë§Œ í¬ì°©
        df['price_volume_synergy'] = (
            df['mom_5d'].clip(-0.3, 0.3) *
            (df['volume_surprise'] - 1).clip(0, 5)
        )
        df['price_volume_synergy'] = df['price_volume_synergy'].clip(-1, 1)

        # === V4.3 ì‹ ê·œ: Amihud Illiquidity ğŸ”¥ ===
        # "ê±°ë˜ëŸ‰ ëŒ€ë¹„ ê°€ê²© ë³€ë™ì´ í° ì¢…ëª© = ìŠ¬ë¦¬í”¼ì§€ ì§€ì˜¥"
        # ë†’ì„ìˆ˜ë¡ ë¹„ìœ ë™ì  â†’ í•„í„°ë§ ëŒ€ìƒ
        df['amihud_illiquidity'] = (
            df['return'].abs() / (df['value'].clip(lower=1e6) / 1e9)  # 10ì–µ ë‹¨ìœ„
        )
        df['amihud_illiquidity'] = grouped['amihud_illiquidity'].rolling(
            20, min_periods=10
        ).mean().reset_index(level=0, drop=True)
        # Percentile rank (ë†’ì„ìˆ˜ë¡ ë¹„ìœ ë™ì )
        df['amihud_rank'] = df.groupby('date')['amihud_illiquidity'].rank(pct=True)

    def _compute_volatility_features(self, df: pd.DataFrame, grouped) -> None:
        """ë³€ë™ì„±/ë¦¬ìŠ¤í¬ í”¼ì²˜ ê³„ì‚° - ìµœì í™” ë²„ì „"""

        # Historical Volatility - ë²¡í„°í™”
        sqrt_252 = np.sqrt(252)
        df['volatility_20d'] = grouped['return'].rolling(20, min_periods=10).std().reset_index(level=0, drop=True) * sqrt_252
        df['volatility_60d'] = grouped['return'].rolling(60, min_periods=30).std().reset_index(level=0, drop=True) * sqrt_252

        # Volatility Ratio (VCP íŒ¨í„´ ê°ì§€)
        df['volatility_ratio'] = df['volatility_20d'] / df['volatility_60d'].clip(lower=0.01)

        # ATR - ë²¡í„°í™”
        prev_close = grouped['closing_price'].shift(1)
        df['tr'] = np.maximum(
            df['high_price'] - df['low_price'],
            np.maximum(
                (df['high_price'] - prev_close).abs(),
                (df['low_price'] - prev_close).abs()
            )
        )
        df['atr_20'] = grouped['tr'].rolling(20, min_periods=10).mean().reset_index(level=0, drop=True)
        df['atr_ratio'] = df['tr'] / df['atr_20'].clip(lower=1)

        # Drawdown from High / Recovery from Low - ë²¡í„°í™”
        df['high_52w'] = grouped['high_price'].rolling(252, min_periods=126).max().reset_index(level=0, drop=True)
        df['low_52w'] = grouped['low_price'].rolling(252, min_periods=126).min().reset_index(level=0, drop=True)

        df['drawdown_from_high'] = df['closing_price'] / df['high_52w'].clip(lower=1) - 1
        df['recovery_from_low'] = df['closing_price'] / df['low_52w'].clip(lower=1) - 1

    def _compute_intuition_features(self, df: pd.DataFrame, grouped) -> None:
        """ë³¸ëŠ¥ ì „ëµ í”¼ì²˜: ì¡´ë‚˜ ì„¼ ë†ˆ + ì¡°ì • -30~50%"""

        # ê³¼ê±°ì˜ ì˜ê´‘ (1ë…„ê°„ ìµœëŒ€ ìƒìŠ¹ë¥ )
        df['past_glory_1y'] = df['high_52w'] / df['low_52w'].clip(lower=1) - 1

        # ì˜ê´‘ ëŒ€ë¹„ í˜„ì¬ ë‚™í­
        df['max_drawdown_from_glory'] = df['drawdown_from_high']

        # Fallen Angel Score (ì¶”ë½í•œ ì²œì‚¬)
        # ê³¼ê±°ì— ì˜ ë‚˜ê°”ëŠ”ë° (glory > 100%) ì§€ê¸ˆ ë§ì´ ë¹ ì§„ (-30% ~ -50%)
        glory_condition = (df['past_glory_1y'] > 1.0).astype(float)
        drawdown_condition = (
            (df['drawdown_from_high'] < -0.30) &
            (df['drawdown_from_high'] > -0.50)
        ).astype(float)
        df['fallen_angel_score'] = glory_condition * drawdown_condition * (
            df['past_glory_1y'] * (-df['drawdown_from_high'])
        )

        # Bounce Potential (ë°˜ë“± ì ì¬ë ¥)
        # ë‚™í­ + ê±°ë˜ëŸ‰ ì¶•ì†Œ + ë³€ë™ì„± ìˆ˜ì¶•
        volume_dryup = (df['volume_trend'] < 0.8).astype(float)
        vol_contraction = (df['volatility_ratio'] < 0.7).astype(float)
        df['bounce_potential'] = (
            df['fallen_angel_score'] *
            (1 + volume_dryup * 0.3) *
            (1 + vol_contraction * 0.3)
        )

        # VCP Score (Volatility Contraction Pattern)
        # ë³€ë™ì„± ìˆ˜ì¶• + ê±°ë˜ëŸ‰ ì¶•ì†Œ + ê°€ê²© íš¡ë³´
        price_stable = ((df['dist_ma_20'].abs() < 0.05)).astype(float)
        df['vcp_score'] = (
            vol_contraction *
            volume_dryup *
            price_stable *
            df['past_glory_1y'].clip(0, 2)
        )

        # ================================================================
        # V3 ì‹ ê·œ: ê²°í•© í”¼ì²˜ (Interaction Features) ğŸ”¥
        # "í”¼ì²˜ë¥¼ ë‚˜ì—´ë§Œ í•˜ì§€ ë§ê³ , ë³¸ëŠ¥ ì „ëµìš© ê²°í•© í”¼ì²˜ë¥¼ ì§ì ‘ ë§Œë“¤ì–´ë¼"
        # ================================================================

        # Glory_Correction_Volume: ì˜ê´‘ * ë‚™í­ * ê±°ë˜ëŸ‰í­ë°œ
        # "ê³¼ê±°ì— í™”ë ¤í–ˆê³  + ì§€ê¸ˆ ì¶©ë¶„íˆ ë¹ ì¡ŒëŠ”ë° + ê±°ë˜ëŸ‰ì´ í„°ì§€ê¸° ì‹œì‘í•œ ë†ˆ"
        df['glory_correction_volume'] = (
            df['past_glory_1y'].clip(0, 5) *
            (-df['drawdown_from_high']).clip(0, 1) *
            (df['volume_surprise'] - 1).clip(0, 10)
        )

        # Fear_Greed_Signal: ê³µí¬ ì† íƒìš• ì‹ í˜¸
        # "ë‚¨ë“¤ì´ ê³µí¬(Volatility)ë¥¼ ëŠë‚„ ë•Œ íƒìš•(Volume)ì„ ë°œê²¬"
        high_volatility = (df['volatility_20d'] > df['volatility_60d']).astype(float)
        volume_spike = (df['volume_surprise'] > 2.0).astype(float)
        price_down = (df['drawdown_from_high'] < -0.20).astype(float)
        df['fear_greed_signal'] = high_volatility * volume_spike * price_down * df['past_glory_1y'].clip(0, 3)

        # Smart_Accumulation: ìŠ¤ë§ˆíŠ¸ë¨¸ë‹ˆ ë§¤ì§‘ ì‹ í˜¸
        # "ì¡°ìš©íˆ ë§¤ì§‘ ì¤‘ - ê±°ë˜ëŸ‰ ì¦ê°€ + ìŠ¤ë§ˆíŠ¸ë¨¸ë‹ˆ ìœ ì… + ë‚™í­ê³¼ëŒ€"
        smart_inflow = (df['smart_money_flow'] > 0.3).astype(float)
        df['smart_accumulation'] = (
            smart_inflow *
            (df['accumulation_index'] + 1).clip(0, 2) *
            (-df['drawdown_from_high']).clip(0, 0.5) *
            df['volume_trend'].clip(0.5, 2)
        )

        # === V4.1 ì‹ ê·œ: í­ë°œ ì§ì „ í¬ì°© ===
        # VCP Breakout Potential (VCP Ã— ê±°ë˜ëŸ‰ ì¶”ì„¸)
        # ë³€ë™ì„±ì´ ì£½ì–´ê°€ë‹¤ê°€ + ê±°ë˜ëŸ‰ì´ ê³ ê°œë¥¼ ë“œëŠ” = í­ë°œ ì„ë°•
        df['vcp_breakout_potential'] = (
            df['vcp_score'].clip(0, 2) *
            (df['volume_trend'] - 0.8).clip(0, 2)  # ê±°ë˜ëŸ‰ì´ í‰ê·  ì´ìƒìœ¼ë¡œ ëŠ˜ì–´ë‚  ë•Œ
        )

    def _compute_traditional_features(self, df: pd.DataFrame, grouped) -> None:
        """ì „í†µì  ê¸°ìˆ  ì§€í‘œ - ìµœì í™” ë²„ì „"""

        # RSI - ë²¡í„°í™”
        delta = grouped['closing_price'].diff()
        gain = delta.where(delta > 0, 0)
        loss = -delta.where(delta < 0, 0)

        avg_gain = grouped.apply(lambda x: gain.loc[x.index].rolling(14).mean()).reset_index(level=0, drop=True) if False else \
                   gain.groupby(df['stock_code']).rolling(14).mean().reset_index(level=0, drop=True)
        avg_loss = loss.groupby(df['stock_code']).rolling(14).mean().reset_index(level=0, drop=True)

        rs = avg_gain / avg_loss.clip(lower=0.001)
        df['rsi_14'] = 100 - (100 / (1 + rs))

        # RSI Divergence (ê°€ê²©ì€ ì‹ ì €ê°€ì¸ë° RSIëŠ” ì•„ë‹Œ ê²½ìš° = ë°˜ë“± ì‹ í˜¸)
        price_new_low = (df['closing_price'] <= df['low_52w'] * 1.05).astype(float)
        rsi_not_low = (df['rsi_14'] > 30).astype(float)
        df['rsi_divergence'] = price_new_low * rsi_not_low

        # Bollinger Bands - ë²¡í„°í™”
        df['bb_mid'] = df['ma_20']
        df['bb_std'] = grouped['closing_price'].rolling(20, min_periods=10).std().reset_index(level=0, drop=True)
        df['bb_upper'] = df['bb_mid'] + 2 * df['bb_std']
        df['bb_lower'] = df['bb_mid'] - 2 * df['bb_std']
        df['bb_position'] = (
            (df['closing_price'] - df['bb_lower']) /
            (df['bb_upper'] - df['bb_lower']).clip(lower=1)
        )

        # BB Squeeze (ë³¼ë¦°ì €ë°´ë“œ ìˆ˜ì¶•) - ë²¡í„°í™”
        df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_mid'].clip(lower=1)
        df['bb_width_avg'] = grouped['bb_width'].rolling(60, min_periods=30).mean().reset_index(level=0, drop=True)
        df['bb_squeeze'] = (df['bb_width'] < df['bb_width_avg'] * 0.8).astype(float)

    def _apply_sector_neutralization(self, df: pd.DataFrame) -> None:
        """ì„¹í„° ì¤‘ë¦½í™”: ëª¨ë“  í”¼ì²˜ë¥¼ ì„¹í„° ë‚´ ìˆœìœ„ë¡œ ë³€í™˜"""

        # Sector-relative momentum (í•µì‹¬!)
        def sector_zscore(x):
            std = x.std()
            if std < 0.01:
                std = 0.01
            return (x - x.mean()) / std

        df['rs_vs_sector_20d'] = df.groupby(['date', 'sector'])['mom_20d'].transform(sector_zscore)

        # ëª¨ë©˜í…€ í”¼ì²˜ë“¤ì„ ì„¹í„° ë‚´ ë­í¬ë¡œ ë³€í™˜
        momentum_cols = ['mom_5d', 'mom_10d', 'mom_20d', 'mom_60d']
        for col in momentum_cols:
            if col in df.columns:
                df[f'{col}_sector_rank'] = df.groupby(['date', 'sector'])[col].rank(pct=True)

    def _cleanup_intermediate_cols(self, df: pd.DataFrame) -> None:
        """ì¤‘ê°„ ê³„ì‚° ì»¬ëŸ¼ ì œê±°"""
        intermediate = [
            'log_return', 'up_day',
            'ma_5', 'ma_20', 'ma_60', 'ma_120',
            'vol_5d', 'vol_20d', 'value_20d',
            'close_location', 'daily_mf',
            'tr', 'atr_20',
            'high_52w', 'low_52w',
            'bb_mid', 'bb_std', 'bb_upper', 'bb_lower', 'bb_width', 'bb_width_avg',
        ]
        df.drop(columns=intermediate, errors='ignore', inplace=True)

    def _load_financial_features_fast(self, start_date: str, end_date: str,
                                       min_market_cap: int) -> pd.DataFrame:
        """V4.1: ê²½ëŸ‰í™”ëœ ì¬ë¬´ í”¼ì²˜ ë¡œë”© (í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì§ì ‘ ì¿¼ë¦¬)

        DB Schema:
        - financial_periods: stock_code, fiscal_date, available_date
        - financial_items_bs_cf: period_id (FK), item_code_normalized, amount_current
        - financial_items_pl: period_id (FK), item_code_normalized, amount_current_ytd
        """
        import sqlite3

        try:
            conn = sqlite3.connect(self.db_path)

            # 1. í•„ìš”í•œ ì¬ë¬´ í•­ëª©ë§Œ ë¡œë“œ (ë¶„ë¦¬ ì¿¼ë¦¬ - ì¸ë±ìŠ¤ í™œìš©)
            available_start = str(int(start_date[:4]) - 1) + start_date[4:]

            # BS/CF ì¿¼ë¦¬ (ìë³¸, ìì‚°)
            bs_query = """
            SELECT fp.stock_code, fp.available_date, fp.industry_name as sector,
                   bs.item_code_normalized as item_code, bs.amount_current as amount
            FROM financial_periods fp
            JOIN financial_items_bs_cf bs ON bs.period_id = fp.id
            WHERE fp.consolidation_type = 'ì—°ê²°'
              AND fp.available_date >= ?
              AND bs.item_code_normalized IN ('ifrs-full_Equity', 'ifrs-full_Assets')
            """
            bs_df = pd.read_sql_query(bs_query, conn, params=[available_start])

            # PL ì¿¼ë¦¬ (ë§¤ì¶œ, ì˜ì—…ì´ìµ, ìˆœì´ìµ)
            pl_query = """
            SELECT fp.stock_code, fp.available_date, fp.industry_name as sector,
                   pl.item_code_normalized as item_code, pl.amount_current_ytd as amount
            FROM financial_periods fp
            JOIN financial_items_pl pl ON pl.period_id = fp.id
            WHERE fp.consolidation_type = 'ì—°ê²°'
              AND fp.available_date >= ?
              AND pl.item_code_normalized IN ('ifrs-full_Revenue', 'dart_OperatingIncomeLoss', 'ifrs-full_ProfitLoss', 'ifrs-full_GrossProfit')
            """
            pl_df = pd.read_sql_query(pl_query, conn, params=[available_start])

            # í•©ì¹˜ê¸°
            fin_df = pd.concat([bs_df, pl_df], ignore_index=True)

            if len(fin_df) == 0:
                conn.close()
                return None

            # Pivot: item_code â†’ columns
            fin_pivot = fin_df.pivot_table(
                index=['stock_code', 'available_date', 'sector'],
                columns='item_code',
                values='amount',
                aggfunc='first'
            ).reset_index()

            # Rename columns
            rename_map = {
                'ifrs-full_Equity': 'equity',
                'ifrs-full_Assets': 'assets',
                'ifrs-full_Revenue': 'revenue',
                'dart_OperatingIncomeLoss': 'operating_income',
                'ifrs-full_ProfitLoss': 'net_income',
                'ifrs-full_GrossProfit': 'gross_profit',
            }
            fin_pivot = fin_pivot.rename(columns=rename_map)
            fin_pivot['available_date'] = pd.to_datetime(fin_pivot['available_date'], format='%Y%m%d')

            # 2. Daily prices ë¡œë“œ (market cap filter)
            price_query = """
            SELECT dp.stock_code, dp.date, dp.market_cap, s.current_sector_type as sector_price
            FROM daily_prices dp
            JOIN stocks s ON dp.stock_code = s.stock_code
            WHERE dp.date >= ? AND dp.date <= ? AND dp.market_cap >= ?
            ORDER BY dp.stock_code, dp.date
            """
            price_df = pd.read_sql_query(price_query, conn, params=[start_date, end_date, min_market_cap])
            conn.close()

            if len(price_df) == 0:
                return None

            price_df['date'] = pd.to_datetime(price_df['date'], format='%Y%m%d')

            # 3. Fast forward-fill approach (merge_asof ëŒ€ì‹ )
            # ê° stockì˜ ë§ˆì§€ë§‰ ì¬ë¬´ ë°ì´í„°ë¥¼ dailyë¡œ forward-fill
            price_df = price_df.sort_values(['stock_code', 'date']).reset_index(drop=True)
            fin_pivot = fin_pivot.sort_values(['stock_code', 'available_date']).reset_index(drop=True)

            # ì¬ë¬´ ë°ì´í„°ë¥¼ ì¼ë³„ë¡œ í™•ì¥ (available_dateë¥¼ date ì»¬ëŸ¼ìœ¼ë¡œ)
            fin_pivot = fin_pivot.rename(columns={'available_date': 'date'})
            fin_cols = ['equity', 'assets', 'revenue', 'operating_income', 'net_income']
            fin_cols = [c for c in fin_cols if c in fin_pivot.columns]

            # Merge and forward fill within each stock
            merged = price_df.merge(
                fin_pivot[['stock_code', 'date'] + fin_cols],
                on=['stock_code', 'date'],
                how='left'
            )
            merged = merged.sort_values(['stock_code', 'date'])

            # Forward fill financial data within each stock (data leakage fix)
            # Financial data should only be available after the available_date
            for col in fin_cols:
                # Only forward fill if the financial data is available (not NaN)
                merged[col] = merged.groupby('stock_code')[col].ffill()

            # 4. ë¹„ìœ¨ ê³„ì‚°
            merged['roe'] = merged['net_income'] / merged['equity'].clip(lower=1)
            merged['operating_margin'] = merged['operating_income'] / merged['revenue'].clip(lower=1)

            # 5. YoY ê³„ì‚° (252 ê±°ë˜ì¼ â‰ˆ 1ë…„)
            merged = merged.sort_values(['stock_code', 'date'])
            grouped = merged.groupby('stock_code')
            merged['revenue_prev'] = grouped['revenue'].shift(252)
            merged['revenue_yoy'] = (merged['revenue'] - merged['revenue_prev']) / merged['revenue_prev'].abs().clip(lower=1)

            # V5: GP/A (Gross Profit / Assets)
            if 'gross_profit' in merged.columns and 'assets' in merged.columns:
                merged['gp_over_assets'] = merged['gross_profit'] / merged['assets'].clip(lower=1)

            # V5: ROE Delta YoY (shift 252 = 1 year)
            if 'roe' in merged.columns:
                merged['roe_delta_yoy'] = merged['roe'] - grouped['roe'].shift(252)

            # V5: P/B Sector Z-Score (market_cap / equity, sector z-score)
            if 'market_cap' in merged.columns and 'equity' in merged.columns:
                merged['pb_ratio'] = merged['market_cap'] / merged['equity'].clip(lower=1)

                def _sector_zscore(x):
                    std = x.std()
                    if std < 0.01:
                        std = 0.01
                    return (x - x.mean()) / std

                if 'sector' in merged.columns or 'sector_price' in merged.columns:
                    sector_col = 'sector_price' if 'sector_price' in merged.columns else 'sector'
                    merged['pb_sector_zscore'] = merged.groupby(['date', sector_col])['pb_ratio'].transform(_sector_zscore)
                else:
                    merged['pb_sector_zscore'] = merged.groupby('date')['pb_ratio'].transform(_sector_zscore)
                merged['pb_sector_zscore'] = merged['pb_sector_zscore'].clip(-3, 3)
                merged.drop(columns=['pb_ratio'], errors='ignore', inplace=True)

            # inf ì²˜ë¦¬
            for col in ['roe', 'operating_margin', 'revenue_yoy',
                         'gp_over_assets', 'roe_delta_yoy', 'pb_sector_zscore']:
                if col in merged.columns:
                    merged[col] = merged[col].replace([np.inf, -np.inf], np.nan)

            # sector ì •ë¦¬
            merged['sector'] = merged.get('sector_price', merged.get('sector'))
            merged = merged.drop(columns=['sector_price', 'available_date'], errors='ignore')

            # dateë¥¼ ë¬¸ìì—´ í¬ë§·ìœ¼ë¡œ ë³€í™˜ (merge í˜¸í™˜ì„±)
            merged['date'] = merged['date'].dt.strftime('%Y%m%d')

            self.logger.info(f"Fast loaded {len(merged):,} financial records")
            return merged

        except Exception as e:
            self.logger.warning(f"Fast financial loading failed: {e}")
            import traceback
            traceback.print_exc()
            return None

    def load_financial_features(self, start_date: str, end_date: str,
                                min_market_cap: int = 500000000000) -> pd.DataFrame:
        """Load financial features with Delta calculations - V4.1 ê²½ëŸ‰í™” ë²„ì „."""
        self.logger.info("Loading financial features (lightweight V4.1)...")

        # V4.1: ì§ì ‘ DBì—ì„œ í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ë¡œë“œ (143ì´ˆ â†’ 5ì´ˆ)
        fin_df = self._load_financial_features_fast(start_date, end_date, min_market_cap)
        if fin_df is not None and len(fin_df) > 0:
            return fin_df

        # Fallback: ê¸°ì¡´ ë°©ì‹
        self.logger.info("Falling back to full FinancialFeatureGenerator...")
        with FinancialFeatureGenerator(self.db_path) as fin_gen:
            fin_df = fin_gen.generate_features(
                start_date=start_date,
                end_date=end_date,
                min_market_cap=min_market_cap,
                include_ranks=True,
                include_missing_indicators=True
            )

        # Add Delta features (QoQ changes)
        fin_df = fin_df.sort_values(['stock_code', 'date'])
        grouped = fin_df.groupby('stock_code')

        # ROE Delta (ë¶„ê¸° ë³€í™”)
        if 'roe' in fin_df.columns:
            fin_df['roe_prev'] = grouped['roe'].shift(63)  # ~1ë¶„ê¸° ì „
            fin_df['roe_delta_qoq'] = fin_df['roe'] - fin_df['roe_prev']

            # V3 ì‹ ê·œ: ROE ì„¹í„° ëŒ€ë¹„ Z-score
            # "ì ˆëŒ€ê°’ Rankê°€ ì•„ë‹ˆë¼, ì„¹í„° ë‚´ì—ì„œ ì–¼ë§ˆë‚˜ íŠ€ì–´ë‚˜ì™”ëŠ”ì§€"
            def sector_zscore(x):
                mean = x.mean()
                std = x.std()
                if std == 0 or pd.isna(std):
                    return 0
                return (x - mean) / std

            if 'sector' in fin_df.columns:
                fin_df['roe_sector_zscore'] = fin_df.groupby(['date', 'sector'])['roe'].transform(sector_zscore)
                fin_df['roe_sector_zscore'] = fin_df['roe_sector_zscore'].clip(-3, 3)
            else:
                fin_df['roe_sector_zscore'] = fin_df.groupby('date')['roe'].transform(sector_zscore)
                fin_df['roe_sector_zscore'] = fin_df['roe_sector_zscore'].clip(-3, 3)

        # Revenue Growth Acceleration
        if 'revenue_yoy' in fin_df.columns:
            fin_df['revenue_yoy_prev'] = grouped['revenue_yoy'].shift(63)
            fin_df['revenue_growth_accel'] = fin_df['revenue_yoy'] - fin_df['revenue_yoy_prev']

        # Margin Improvement
        if 'operating_margin' in fin_df.columns:
            fin_df['margin_prev'] = grouped['operating_margin'].shift(63)
            fin_df['margin_improvement'] = fin_df['operating_margin'] - fin_df['margin_prev']

        # Convert date
        fin_df['date'] = fin_df['date'].dt.strftime('%Y%m%d')

        self.logger.info(f"Loaded {len(fin_df):,} financial records with deltas")
        return fin_df

    def merge_financial_features(self, tech_df: pd.DataFrame,
                                 fin_df: pd.DataFrame) -> pd.DataFrame:
        """Merge technical and financial features."""
        # V4.1: ì¬ë¬´ í”¼ì²˜ 4ê°œë¥¼ ë‹¨ì¼ financial_quality_indexë¡œ ì••ì¶•
        # V5: ì¶”ê°€ ì¬ë¬´ í”¼ì²˜ í¬í•¨
        _v5_fin_cols = ['gp_over_assets', 'roe_delta_yoy', 'pb_sector_zscore']
        raw_cols = ['stock_code', 'date'] + [
            col for col in self._FUNDAMENTAL_RAW + _v5_fin_cols
            if col in fin_df.columns
        ]
        fin_subset = fin_df[raw_cols].drop_duplicates(subset=['stock_code', 'date'])

        merged = pd.merge(
            tech_df,
            fin_subset,
            on=['stock_code', 'date'],
            how='left'
        )

        # Fill missing raw features
        for col in self._FUNDAMENTAL_RAW + _v5_fin_cols:
            if col in merged.columns:
                merged[col] = merged.groupby('date')[col].transform(
                    lambda x: x.fillna(x.median())
                )

        # === V4.1: Financial Quality Index ê³„ì‚° ===
        # ê° ì¬ë¬´ í”¼ì²˜ë¥¼ 0~1 ë­í¬ë¡œ ë³€í™˜ í›„ í‰ê· 
        rank_cols = []
        for col in self._FUNDAMENTAL_RAW:
            if col in merged.columns:
                rank_col = f'{col}_rank'
                merged[rank_col] = merged.groupby('date')[col].rank(pct=True)
                rank_cols.append(rank_col)

        if rank_cols:
            merged['financial_quality_index'] = merged[rank_cols].mean(axis=1)
            # ì„ì‹œ ë­í¬ ì»¬ëŸ¼ ì œê±°
            merged.drop(columns=rank_cols, inplace=True)
        else:
            merged['financial_quality_index'] = 0.5  # fallback

        return merged

    def add_forward_returns(self, df: pd.DataFrame,
                            horizons: List[int] = None) -> pd.DataFrame:
        """
        Add forward returns using Open-to-Open pricing (ì •ì„).

        Signal: Tì¼ ì¢…ê°€ ë°ì´í„°ê¹Œì§€ ë³´ê³  ìƒì„±
        Buy:  T+1ì¼ ì‹œê°€ (Open)  â† Tì¼ ì¢…ê°€ë¡œëŠ” ë§¤ìˆ˜ ë¶ˆê°€
        Sell: T+1+hì¼ ì‹œê°€ (Open)

        V4.3: Residual Return ì¶”ê°€ (Beta-adjusted)
        - De Prado (2018): "Alphaë¥¼ ì°¾ìœ¼ë ¤ë©´ Betaë¥¼ ì œê±°í•˜ë¼"
        """
        horizons = horizons or [21]

        df = df.sort_values(['stock_code', 'date']).copy()
        grouped = df.groupby('stock_code')

        # V4.3: ì‹œì¥ ìˆ˜ìµë¥  (Beta ì¶”ì •ìš©)
        market_ret_series = df.groupby('date')['return'].transform('median')
        df['_market_return_daily'] = market_ret_series

        # ================================================================
        # Rolling Beta ê³„ì‚° (í•œ ë²ˆë§Œ, ëª¨ë“  horizonì—ì„œ ê³µìœ )
        # ================================================================
        def _calc_stock_beta(stock_df):
            """ë‹¨ì¼ ì¢…ëª©ì˜ rolling beta ê³„ì‚°"""
            if len(stock_df) < 60:
                return pd.Series(np.nan, index=stock_df.index)
            market_rets = market_ret_series.loc[stock_df.index]
            stock_rets = stock_df['return']
            rolling_cov = stock_rets.rolling(252, min_periods=60).cov(market_rets)
            rolling_var = market_rets.rolling(252, min_periods=60).var()
            return (rolling_cov / rolling_var.clip(lower=1e-8)).clip(-3, 3)

        # Stock-by-stock beta (groupby.apply)
        df['rolling_beta'] = grouped.apply(
            lambda g: _calc_stock_beta(g)
        ).reset_index(level=0, drop=True)
        df['rolling_beta'] = df['rolling_beta'].fillna(1.0)

        # V5: Rolling Beta 60d (short-window beta)
        def _calc_stock_beta_60d(stock_df):
            """ë‹¨ì¼ ì¢…ëª©ì˜ 60ì¼ rolling beta ê³„ì‚°"""
            if len(stock_df) < 30:
                return pd.Series(np.nan, index=stock_df.index)
            market_rets = market_ret_series.loc[stock_df.index]
            stock_rets = stock_df['return']
            rolling_cov = stock_rets.rolling(60, min_periods=30).cov(market_rets)
            rolling_var = market_rets.rolling(60, min_periods=30).var()
            return (rolling_cov / rolling_var.clip(lower=1e-8)).clip(-3, 3)

        df['rolling_beta_60d'] = grouped.apply(
            lambda g: _calc_stock_beta_60d(g)
        ).reset_index(level=0, drop=True)
        df['rolling_beta_60d'] = df['rolling_beta_60d'].fillna(1.0)
        gc.collect()

        # ================================================================
        # Horizonë³„ forward return + alpha + residual + ranks
        # ================================================================
        for h in horizons:
            col_name = f'forward_return_{h}d'

            # Open-to-Open: T+1 ì‹œê°€ â†’ T+1+h ì‹œê°€ (look-ahead bias ë°©ì§€)
            open_t1 = grouped['opening_price'].shift(-1)
            open_t1_h = grouped['opening_price'].shift(-1 - h)
            df[col_name] = (open_t1_h - open_t1) / open_t1
            df[col_name] = df[col_name].clip(-0.50, 0.50)

            # Alpha (ì‹œì¥ ëŒ€ë¹„ ì´ˆê³¼ìˆ˜ìµë¥ )
            forward_market = df.groupby('date')[col_name].transform('median')
            alpha_col = f'forward_alpha_{h}d'
            df[alpha_col] = df[col_name] - forward_market

            # Residual = Actual - (Beta Ã— Market_Return)
            residual_col = f'forward_residual_{h}d'
            df[residual_col] = df[col_name] - (df['rolling_beta'] * forward_market)

            # Target Ranks (ëª¨ë‘ loop ì•ˆì—ì„œ ê³„ì‚°)
            df[f'target_rank_{h}d'] = df.groupby('date')[col_name].rank(pct=True)
            df[f'target_alpha_rank_{h}d'] = df.groupby('date')[alpha_col].rank(pct=True)
            df[f'target_residual_rank_{h}d'] = df.groupby('date')[residual_col].rank(pct=True)
            df[f'target_sector_rank_{h}d'] = df.groupby(['date', 'sector'])[col_name].rank(pct=True)

        # ì„ì‹œ ì»¬ëŸ¼ ì •ë¦¬
        df.drop(columns=['_market_return_daily'], errors='ignore', inplace=True)

        return df

    def filter_universe(self, df: pd.DataFrame,
                        min_price: int = 1000,
                        min_market_cap: int = 500000000000,
                        min_value: int = 10000000000) -> pd.DataFrame:
        """Filter stock universe."""
        original_len = len(df)

        df = df[df['closing_price'] >= min_price]
        df = df[df['market_cap'] >= min_market_cap]
        df = df[df['value'] >= min_value]

        # Drop rows with NaN in key features
        key_features = ['mom_20d', 'volume_surprise', 'volatility_20d']
        key_features = [f for f in key_features if f in df.columns]
        df = df.dropna(subset=key_features)

        self.logger.info(f"Filtered: {original_len:,} -> {len(df):,} rows")
        return df

    def prepare_ml_data(self, start_date: str, end_date: str,
                        target_horizon: int = 21,  # V2: ê¸°ë³¸ 21ì¼
                        min_market_cap: int = 500000000000,
                        include_fundamental: bool = True,
                        include_macro: bool = False,
                        use_cache: bool = True) -> pd.DataFrame:
        """
        Full pipeline for ML data preparation.

        Args:
            include_macro: V4 ë§¤í¬ë¡œ í”¼ì²˜ í¬í•¨ ì—¬ë¶€ (Regime Detection)
            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ True - ì†ë„ í–¥ìƒ)
        """
        import hashlib
        import os
        import time

        # ìºì‹œ íŒŒì¼ëª… ìƒì„±
        cache_key = f"{start_date}_{end_date}_{target_horizon}_{min_market_cap}_{include_fundamental}_{include_macro}"
        cache_hash = hashlib.md5(cache_key.encode()).hexdigest()[:8]
        cache_file = f".cache/features_{cache_hash}.parquet"

        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(".cache", exist_ok=True)

        # ìºì‹œ í™•ì¸
        if use_cache and os.path.exists(cache_file):
            cache_mtime = os.path.getmtime(cache_file)
            db_mtime = os.path.getmtime(self.db_path)
            if cache_mtime > db_mtime:
                self.logger.info(f"Loading cached features from {cache_file}")
                return pd.read_parquet(cache_file)

        pipeline_t0 = time.time()

        # 1ë…„ ë²„í¼
        buffer_start = str(int(start_date[:4]) - 1) + start_date[4:]

        # Step 1: Load raw data (SQL)
        t0 = time.time()
        self.logger.info("[1/5] Loading raw data...")
        df = self.load_raw_data(buffer_start, end_date)
        print(f'  â± [1/5] Raw data SQL load: {time.time()-t0:.1f}s ({len(df):,} rows)')

        # Step 2: Compute technical features
        t0 = time.time()
        self.logger.info("[2/5] Computing technical features...")
        df = self.compute_features(df)
        print(f'  â± [2/5] Technical features: {time.time()-t0:.1f}s')

        # Step 3: Load and merge financial features
        if include_fundamental:
            t0 = time.time()
            self.logger.info("[3/5] Loading financial features...")
            fin_df = self.load_financial_features(start_date, end_date, min_market_cap)
            t_load = time.time() - t0
            t0 = time.time()
            df = self.merge_financial_features(df, fin_df)
            t_merge = time.time() - t0
            print(f'  â± [3/5] Financial features: load={t_load:.1f}s, merge={t_merge:.1f}s')

        # Step 4: Add forward returns (+ rolling beta)
        t0 = time.time()
        self.logger.info("[4/5] Computing forward returns...")
        df = self.add_forward_returns(df, [target_horizon])
        print(f'  â± [4/5] Forward returns + beta: {time.time()-t0:.1f}s')

        # Step 5: Filter universe
        t0 = time.time()
        df = self.filter_universe(df, min_market_cap=min_market_cap)

        # Filter to requested date range
        df = df[df['date'] >= start_date]
        print(f'  â± [5/5] Filter universe: {time.time()-t0:.1f}s ({len(df):,} rows)')

        # Step 6: V4 - Add macro features (Regime Detection)
        if include_macro:
            t0 = time.time()
            self.logger.info("[6] Adding macro features...")
            df = self._add_macro_features(df, start_date, end_date)
            print(f'  â± [6] Macro features: {time.time()-t0:.1f}s')

        # ìºì‹œ ì €ì¥
        if use_cache:
            t0 = time.time()
            df.to_parquet(cache_file, index=False)
            print(f'  â± Cache save: {time.time()-t0:.1f}s')
            self.logger.info(f"Cached features to {cache_file}")

        print(f'  â± Total pipeline: {time.time()-pipeline_t0:.1f}s')

        # Feature count
        tech_count = len([c for c in df.columns if c in
                         self.MOMENTUM_FEATURES + self.VOLUME_FEATURES +
                         self.VOLATILITY_FEATURES + self.INTUITION_FEATURES +
                         self.TRADITIONAL_FEATURES])
        fund_count = len([c for c in self.FUNDAMENTAL_FEATURES if c in df.columns])
        macro_count = len([c for c in self.MACRO_FEATURES if c in df.columns])

        version = "V4" if include_macro else "V3"
        self.logger.info(f"{version} ML data: {len(df):,} samples")
        self.logger.info(f"  Technical: {tech_count}, Fundamental: {fund_count}, Macro: {macro_count}")
        if tech_count + fund_count > 0:
            self.logger.info(f"  Fundamental ratio: {fund_count/(tech_count+fund_count)*100:.0f}%")

        return df

    def _add_macro_features(self, df: pd.DataFrame,
                            start_date: str, end_date: str) -> pd.DataFrame:
        """
        V4: ë§¤í¬ë¡œ Regime Detection í”¼ì²˜ ì¶”ê°€

        "2021~2022ë…„ í­ë½ì¥ì„ í”¼í•˜ê¸° ìœ„í•œ ì‹œì¥ ì˜¨ë„ê³„"
        """
        self.logger.info("Adding V4 macro features (Regime Detection)...")

        # MacroFeatureEngineer ì´ˆê¸°í™”
        macro_eng = MacroFeatureEngineer(self.db_path)

        # ë‚ ì§œë³„ ë§¤í¬ë¡œ í”¼ì²˜ ì¤€ë¹„
        macro_df = macro_eng.prepare_macro_features(start_date, end_date)

        # ì¢…ëª© ë°ì´í„°ì— ë³‘í•©
        df = pd.merge(df, macro_df, on='date', how='left')


        # ë³µí•© í”¼ì²˜ ê³„ì‚°
        df = self._compute_macro_composite_features(df)

        # Forward fill (ì£¼ë§/íœ´ì¼ ë°ì´í„°)
        macro_cols = [c for c in self.MACRO_FEATURES if c in df.columns]
        df[macro_cols] = df.groupby('stock_code')[macro_cols].ffill()

        self.logger.info(f"Added {len(macro_cols)} macro features")
        return df

    def _compute_macro_composite_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        V4 ë³µí•© ë§¤í¬ë¡œ í”¼ì²˜ ê³„ì‚°

        - macro_risk_score: ì¢…í•© ë¦¬ìŠ¤í¬ ì ìˆ˜
        - regime_momentum_interaction: regimeê³¼ ëª¨ë©˜í…€ ìƒí˜¸ì‘ìš©
        """
        df = df.copy()

        # === Macro Risk Score ===
        # ë†’ì„ìˆ˜ë¡ ìœ„í—˜ (ìŒìˆ˜ regime + ë†’ì€ fear + ì–‘ìˆ˜ dollar + ì–‘ìˆ˜ bond spread)
        risk_components = []

        if 'market_regime_score' in df.columns:
            # Regimeì´ ìŒìˆ˜ë©´ ë¦¬ìŠ¤í¬ ë†’ìŒ (ì •ê·œí™”)
            risk_components.append(-df['market_regime_score'].clip(-0.3, 0.3) / 0.3)

        if 'fear_index_delta' in df.columns:
            # VKOSPI ê¸‰ë“±í•˜ë©´ ë¦¬ìŠ¤í¬ ë†’ìŒ
            risk_components.append(df['fear_index_delta'].clip(-10, 10) / 10)

        if 'dollar_impact' in df.columns:
            # ë‹¬ëŸ¬ ê¸‰ë“±í•˜ë©´ ë¦¬ìŠ¤í¬ ë†’ìŒ
            risk_components.append(df['dollar_impact'].clip(-0.1, 0.1) / 0.1)

        if 'bond_stock_spread' in df.columns:
            # ì±„ê¶Œ ì„ í˜¸ ë†’ìœ¼ë©´ ë¦¬ìŠ¤í¬ ë†’ìŒ
            risk_components.append(df['bond_stock_spread'].clip(-0.1, 0.1) / 0.1)

        if risk_components:
            df['macro_risk_score'] = sum(risk_components) / len(risk_components)
        else:
            df['macro_risk_score'] = 0

        # === Regime-Momentum Interaction ===
        # "ì¢‹ì€ ì¥ì—ì„œ ì¢‹ì€ ëª¨ë©˜í…€ì„ ê°€ì§„ ë†ˆ" vs "ë‚˜ìœ ì¥ì—ì„œ ëª¨ë©˜í…€ë§Œ ì¢‹ì€ ë†ˆ"
        if 'market_regime_score' in df.columns and 'mom_20d' in df.columns:
            df['regime_momentum_interaction'] = (
                df['market_regime_score'].clip(-0.2, 0.2) *
                df['mom_20d'].clip(-0.5, 0.5)
            )
        else:
            df['regime_momentum_interaction'] = 0

        return df


# =============================================================================
# CLI Entry Point
# =============================================================================

if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser(description='Feature Engineering V4')
    parser.add_argument('--version', choices=['v3', 'v4'], default='v4',
                        help='Feature version (v3: no macro, v4: with macro)')
    parser.add_argument('--start-date', default='20200101', help='Start date')
    parser.add_argument('--end-date', default='20260128', help='End date')
    parser.add_argument('--output', default=None, help='Output parquet file')
    args = parser.parse_args()

    logging.basicConfig(level=logging.INFO)

    fe = FeatureEngineer('krx_stock_data.db')
    include_macro = (args.version == 'v4')

    print(f"\n{'='*60}")
    print(f"Feature Engineering {args.version.upper()}")
    print(f"{'='*60}")

    df = fe.prepare_ml_data(
        start_date=args.start_date,
        end_date=args.end_date,
        target_horizon=21,
        include_fundamental=True,
        include_macro=include_macro
    )

    # í”¼ì²˜ í†µê³„
    print(f"\n{'='*60}")
    print(f"Feature Summary")
    print(f"{'='*60}")

    tech_features = [c for c in df.columns if c in
                     fe.MOMENTUM_FEATURES + fe.VOLUME_FEATURES +
                     fe.VOLATILITY_FEATURES + fe.INTUITION_FEATURES +
                     fe.TRADITIONAL_FEATURES]
    fund_features = [c for c in fe.FUNDAMENTAL_FEATURES if c in df.columns]
    macro_features = [c for c in fe.MACRO_FEATURES if c in df.columns]

    print(f"\nTechnical features ({len(tech_features)}):")
    for f in tech_features[:10]:
        print(f"  - {f}")
    if len(tech_features) > 10:
        print(f"  ... and {len(tech_features)-10} more")

    print(f"\nFundamental features ({len(fund_features)}):")
    for f in fund_features:
        print(f"  - {f}")

    if include_macro:
        print(f"\nMacro features ({len(macro_features)}):")
        for f in macro_features:
            print(f"  - {f}")

        # ë§¤í¬ë¡œ í”¼ì²˜ í†µê³„
        print(f"\nMacro Feature Statistics:")
        for col in macro_features:
            if col in df.columns:
                print(f"  {col}: mean={df[col].mean():.4f}, "
                      f"std={df[col].std():.4f}, "
                      f"null%={df[col].isna().mean()*100:.1f}%")

    print(f"\nì´ í”¼ì²˜ ìˆ˜: {len(tech_features) + len(fund_features) + len(macro_features)}")
    print(f"ë°ì´í„° í¬ê¸°: {len(df):,} rows")
    print(f"ì¢…ëª© ìˆ˜: {df['stock_code'].nunique()}")
    print(f"ë‚ ì§œ ë²”ìœ„: {df['date'].min()} ~ {df['date'].max()}")

    # ì €ì¥
    if args.output:
        df.to_parquet(args.output, index=False)
        print(f"\nSaved to {args.output}")
