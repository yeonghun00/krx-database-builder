#!/usr/bin/env python3
"""
V3 QEPM ë°±í…ŒìŠ¤íŠ¸ (Quantitative Equity Portfolio Management)

Usage:
    python3 run_backtest.py                         # ê¸°ë³¸ (quintile)
    python3 run_backtest.py --top 10                # ìƒìœ„ 10ê°œ ì¢…ëª©
    python3 run_backtest.py --qepm                  # ğŸ”¥ QEPM ëª¨ë“œ (ê¶Œì¥)
    python3 run_backtest.py --qepm --top 20         # QEPM + Top 20

QEPM ëª¨ë“œ:
    - 63ì¼(3ê°œì›”) í˜¸ë¼ì´ì¦Œ
    - Alpha íƒ€ê²Ÿ (ì‹œì¥ ëŒ€ë¹„ ì´ˆê³¼ìˆ˜ìµ)
    - ì„¹í„°ë‹¹ ìµœëŒ€ 3ì¢…ëª©
    - ë³€ë™ì„± ì—­ê°€ì¤‘ ë°°ë¶„
    - íšŒì „ìœ¨ ì œì–´ (20% ë²„í¼)
"""

import warnings
warnings.filterwarnings('ignore')

import logging
logging.basicConfig(level=logging.WARNING)

import pandas as pd
import numpy as np
import argparse
from scipy import stats
from datetime import datetime
from ml.features import FeatureEngineer
from ml.model import MLRanker

parser = argparse.ArgumentParser()
parser.add_argument('--top', type=int, default=0, help='ìƒìœ„ Nê°œ ì¢…ëª©ë§Œ (0=quintile)')
parser.add_argument('--horizon', type=int, default=21, help='ë³´ìœ  ê¸°ê°„ (ì¼): 21, 63, 126')
parser.add_argument('--slippage', type=float, default=0.5, help='í¸ë„ ìŠ¬ë¦¬í”¼ì§€ %% (ê¸°ë³¸ 0.5%%)')
parser.add_argument('--qepm', action='store_true', help='ğŸ”¥ QEPM ëª¨ë“œ (63ì¼, Alpha, ì„¹í„°ì œí•œ, ë³€ë™ì„±ê°€ì¤‘)')
parser.add_argument('--max-sector', type=int, default=3, help='ì„¹í„°ë‹¹ ìµœëŒ€ ì¢…ëª© ìˆ˜ (QEPM)')
parser.add_argument('--turnover-buffer', type=float, default=0.2, help='íšŒì „ìœ¨ ë²„í¼ (ê¸°ì¡´ ì¢…ëª© êµì²´ ê¸°ì¤€)')
args = parser.parse_args()

# QEPM ëª¨ë“œë©´ ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ
if args.qepm:
    args.horizon = 63  # 3ê°œì›”
    if args.top == 0:
        args.top = 20  # ê¸°ë³¸ 20ì¢…ëª©

print('=' * 70)
if args.qepm:
    print('ğŸ¦ V3 QEPM ë°±í…ŒìŠ¤íŠ¸ (ê¸°ê´€ê¸‰ í¬íŠ¸í´ë¦¬ì˜¤)')
else:
    print('ğŸš€ V3 ëª¨ë¸ ë°±í…ŒìŠ¤íŠ¸')
print(f'   {datetime.now().strftime("%Y-%m-%d %H:%M")}')
print('=' * 70)

# ============================================================================
# ì„¤ì •
# ============================================================================
HORIZON = args.horizon
TOP_N = args.top  # 0ì´ë©´ quintile ì‚¬ìš©
SLIPPAGE = args.slippage / 100  # í¸ë„ ìŠ¬ë¦¬í”¼ì§€ (0.5% -> 0.005)
ROUND_TRIP_COST = SLIPPAGE * 2  # ì™•ë³µ ë¹„ìš©
TRAIN_YEARS = 3
BUFFER_MONTHS = 2  # horizon ëŒ€ë¹„ ë²„í¼
QEPM_MODE = args.qepm
MAX_PER_SECTOR = args.max_sector
TURNOVER_BUFFER = args.turnover_buffer

print(f'\nì„¤ì •:')
print(f'  - ëª¨ë“œ: {"ğŸ¦ QEPM (ê¸°ê´€ê¸‰)" if QEPM_MODE else "ì¼ë°˜"}')
print(f'  - Target Horizon: {HORIZON}ì¼ ({HORIZON//21}ê°œì›”)')
print(f'  - í¬íŠ¸í´ë¦¬ì˜¤: {"ìƒìœ„ " + str(TOP_N) + "ê°œ" if TOP_N > 0 else "Quintile (ìƒìœ„ 20%)"}')
print(f'  - ìŠ¬ë¦¬í”¼ì§€: {args.slippage}% (ì™•ë³µ {args.slippage*2}%)')
if QEPM_MODE:
    print(f'  - ì„¹í„° ì œí•œ: ì„¹í„°ë‹¹ ìµœëŒ€ {MAX_PER_SECTOR}ì¢…ëª©')
    print(f'  - íšŒì „ìœ¨ ë²„í¼: {TURNOVER_BUFFER*100:.0f}% (êµì²´ ê¸°ì¤€)')
    print(f'  - íƒ€ê²Ÿ: Alpha (ì‹œì¥ ëŒ€ë¹„ ì´ˆê³¼ìˆ˜ìµ)')
    print(f'  - ê°€ì¤‘ì¹˜: ë³€ë™ì„± ì—­ê°€ì¤‘ (Risk Parity)')
print(f'  - í•™ìŠµ ê¸°ê°„: {TRAIN_YEARS}ë…„')

# ============================================================================
# ë°ì´í„° ë¡œë“œ
# ============================================================================
print('\n[1/3] ë°ì´í„° ë¡œë“œ ì¤‘...')

fe = FeatureEngineer('krx_stock_data.db')
df = fe.prepare_ml_data(
    start_date='20180101',  # ë” ê¸´ ê¸°ê°„
    end_date='20260128',
    target_horizon=HORIZON,
    min_market_cap=500_000_000_000,
    include_fundamental=True
)

# í”¼ì²˜ ë¶„ë¥˜
momentum_features = [c for c in fe.MOMENTUM_FEATURES if c in df.columns]
volume_features = [c for c in fe.VOLUME_FEATURES if c in df.columns]
volatility_features = [c for c in fe.VOLATILITY_FEATURES if c in df.columns]
intuition_features = [c for c in fe.INTUITION_FEATURES if c in df.columns]
traditional_features = [c for c in fe.TRADITIONAL_FEATURES if c in df.columns]
fund_features = [c for c in fe.FUNDAMENTAL_FEATURES if c in df.columns]

all_features = (momentum_features + volume_features + volatility_features +
                intuition_features + traditional_features + fund_features)

tech_count = len(momentum_features + volume_features + volatility_features +
                 intuition_features + traditional_features)

print(f'  ì´ ë°ì´í„°: {len(df):,} rows')
print(f'  í”¼ì²˜ êµ¬ì„±:')
print(f'    - ëª¨ë©˜í…€: {len(momentum_features)}ê°œ')
print(f'    - ìˆ˜ê¸‰: {len(volume_features)}ê°œ')
print(f'    - ë³€ë™ì„±: {len(volatility_features)}ê°œ')
print(f'    - ë³¸ëŠ¥ì „ëµ: {len(intuition_features)}ê°œ')
print(f'    - ì „í†µì§€í‘œ: {len(traditional_features)}ê°œ')
print(f'    - ì¬ë¬´: {len(fund_features)}ê°œ')
print(f'  ì¬ë¬´ ë¹„ì¤‘: {len(fund_features)/len(all_features)*100:.1f}% (ëª©í‘œ: 30-40%)')

# Forward return ì¶”ê°€
df = df.sort_values(['stock_code', 'date'])
df['year'] = df['date'].str[:4].astype(int)
years = sorted(df['year'].unique())

# ============================================================================
# Walk-Forward ë°±í…ŒìŠ¤íŠ¸
# ============================================================================
print('\n[2/3] Walk-Forward ë°±í…ŒìŠ¤íŠ¸...')
print('-' * 70)

# QEPMì€ Alpha íƒ€ê²Ÿ ì‚¬ìš©, ì¼ë°˜ì€ ì ˆëŒ€ ìˆ˜ìµë¥  íƒ€ê²Ÿ
if QEPM_MODE:
    target_col = f'target_alpha_rank_{HORIZON}d'  # Alpha ìˆœìœ„
    return_col = f'forward_alpha_{HORIZON}d'      # Alpha ìˆ˜ìµë¥ 
    if target_col not in df.columns:
        target_col = f'target_rank_{HORIZON}d'    # fallback
        return_col = f'forward_return_{HORIZON}d'
else:
    target_col = f'target_rank_{HORIZON}d'
    return_col = f'forward_return_{HORIZON}d'

all_results = []

# ============================================================================
# QEPM í—¬í¼ í•¨ìˆ˜ë“¤
# ============================================================================

def get_benchmark_return(year, horizon=63):
    """
    DBì—ì„œ KOSPI200 ë²¤ì¹˜ë§ˆí¬ ìˆ˜ìµë¥  ê°€ì ¸ì˜¤ê¸°
    (build_benchmark.pyë¡œ ë¯¸ë¦¬ ìƒì„±í•´ì•¼ í•¨)
    """
    import sqlite3

    try:
        conn = sqlite3.connect('krx_stock_data.db')
        query = f"""
        SELECT AVG(return_{horizon}d) * 100 as avg_return
        FROM benchmark_kospi200
        WHERE date LIKE '{year}%'
          AND return_{horizon}d IS NOT NULL
        """
        result = pd.read_sql_query(query, conn)
        conn.close()

        if len(result) > 0 and result['avg_return'].iloc[0] is not None:
            return result['avg_return'].iloc[0]
    except Exception as e:
        print(f'    âš ï¸ ë²¤ì¹˜ë§ˆí¬ ë¡œë“œ ì‹¤íŒ¨: {e}')

    return 0  # fallback

def select_with_sector_constraint(df, top_n, max_per_sector):
    """ì„¹í„° ì œì•½ ì ìš©í•œ ì¢…ëª© ì„ ì •"""
    df = df.sort_values('pred_rank')
    selected = []
    sector_count = {}

    for _, row in df.iterrows():
        sector = row.get('sector', 'Unknown')
        if sector_count.get(sector, 0) < max_per_sector:
            selected.append(row)
            sector_count[sector] = sector_count.get(sector, 0) + 1
        if len(selected) >= top_n:
            break

    return pd.DataFrame(selected)

def apply_inverse_volatility_weight(df, return_col):
    """ë³€ë™ì„± ì—­ê°€ì¤‘ ì ìš©í•œ ìˆ˜ìµë¥  ê³„ì‚°"""
    if 'volatility_20d' not in df.columns or len(df) == 0:
        return df[return_col].mean()

    vol = df['volatility_20d'].fillna(df['volatility_20d'].median())
    vol = vol.clip(lower=0.1)  # ìµœì†Œ ë³€ë™ì„±
    inv_vol = 1 / vol
    weights = inv_vol / inv_vol.sum()

    return (df[return_col] * weights).sum()

for test_year in years:
    train_start = test_year - TRAIN_YEARS
    train_end = test_year - 1

    # Buffer ì ìš© (horizon ê±°ë˜ì¼ ì „ì— í•™ìŠµ ì¢…ë£Œ)
    buffer_month = 12 - (HORIZON // 21 + BUFFER_MONTHS)
    buffer_month = max(1, min(buffer_month, 10))
    train_cutoff = f'{train_end}{buffer_month:02d}01'

    train_df = df[(df['year'] >= train_start) &
                  (df['year'] <= train_end) &
                  (df['date'] <= train_cutoff)].copy()
    test_df = df[df['year'] == test_year].copy()

    if len(train_df) < 1000 or len(test_df) < 100:
        continue

    # ëª¨ë¸ í•™ìŠµ (QEPMì€ ë‚®ì€ time_decayë¡œ ì•ˆì •ì„± ì¶”êµ¬)
    model = MLRanker(
        feature_cols=all_features,
        target_col=target_col,
        model_type='regressor',
        time_decay=0.5 if QEPM_MODE else 0.7  # QEPMì€ ë” ë³´ìˆ˜ì 
    )
    model.train(train_df)

    # ì˜ˆì¸¡
    test_df['pred_score'] = model.predict(test_df)
    test_df['pred_rank'] = test_df.groupby('date')['pred_score'].rank(ascending=False)

    if TOP_N > 0:
        if QEPM_MODE:
            # QEPM: ì„¹í„° ì œì•½ + ë³€ë™ì„± ê°€ì¤‘
            top_df = test_df.groupby('date').apply(
                lambda x: select_with_sector_constraint(x, TOP_N, MAX_PER_SECTOR)
            ).reset_index(drop=True)

            # ë³€ë™ì„± ì—­ê°€ì¤‘ ìˆ˜ìµë¥ 
            q5_raw = test_df.groupby('date').apply(
                lambda x: apply_inverse_volatility_weight(
                    select_with_sector_constraint(x, TOP_N, MAX_PER_SECTOR),
                    return_col
                )
            ).mean() * 100
        else:
            # ì¼ë°˜: ë‹¨ìˆœ Top N
            top_df = test_df[test_df['pred_rank'] <= TOP_N]
            q5_raw = top_df[return_col].mean() * 100

        bottom_df = test_df[test_df['pred_rank'] > test_df.groupby('date')['pred_rank'].transform('max') - TOP_N]
        q1_raw = bottom_df[return_col].mean() * 100

        # QEPMì€ íšŒì „ìœ¨ ê°ì†Œë¡œ ìŠ¬ë¦¬í”¼ì§€ 1/3
        effective_slippage = ROUND_TRIP_COST / 3 if QEPM_MODE else ROUND_TRIP_COST
        q5 = q5_raw - effective_slippage * 100
        q1 = q1_raw - effective_slippage * 100
        spread = q5 - q1 if not (np.isnan(q5) or np.isnan(q1)) else np.nan
    else:
        # Quintile ìˆ˜ìµë¥ 
        test_df['quintile'] = pd.qcut(
            test_df['pred_rank'], 5,
            labels=[5,4,3,2,1], duplicates='drop'  # 1=top, 5=bottom
        )
        quintile_ret = test_df.groupby('quintile')[return_col].mean() * 100
        q1_raw = quintile_ret.get(1, np.nan)  # Bottom 20%
        q5_raw = quintile_ret.get(5, np.nan)  # Top 20%
        # ìŠ¬ë¦¬í”¼ì§€ ì ìš© (ì™•ë³µ ë¹„ìš© ì°¨ê°)
        q5 = q5_raw - ROUND_TRIP_COST * 100 if not np.isnan(q5_raw) else np.nan
        q1 = q1_raw - ROUND_TRIP_COST * 100 if not np.isnan(q1_raw) else np.nan
        spread = q5 - q1 if not (np.isnan(q5) or np.isnan(q1)) else np.nan

    # IC ê³„ì‚°
    ic, _ = stats.spearmanr(
        test_df['pred_score'].fillna(0),
        test_df[return_col].fillna(0)
    )

    # QEPM: ì‹œì¥(KOSPI200) ìˆ˜ìµë¥  - DBì—ì„œ ê°€ì ¸ì˜¤ê¸°
    if QEPM_MODE:
        market_ret = get_benchmark_return(test_year, HORIZON)
        abs_top = q5 + market_ret  # ì ˆëŒ€ ìˆ˜ìµë¥  = Alpha + ì‹œì¥
        abs_bot = q1 + market_ret
    else:
        market_ret = 0
        abs_top = q5
        abs_bot = q1

    # ì‹¤ì œ í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ìµë¥  ê³„ì‚° (Alpha ì•„ë‹Œ ì ˆëŒ€ ìˆ˜ìµë¥ !)
    abs_return_col = f'forward_return_{HORIZON}d'
    if TOP_N > 0 and abs_return_col in test_df.columns:
        if QEPM_MODE:
            # QEPM: ì„¹í„° ì œì•½ ì ìš©
            top_selected = test_df.groupby('date').apply(
                lambda x: select_with_sector_constraint(x, TOP_N, MAX_PER_SECTOR)
            ).reset_index(drop=True)
            portfolio_return = top_selected[abs_return_col].mean() * 100 - ROUND_TRIP_COST * 100
        else:
            # ì¼ë°˜: Top N í‰ê· 
            top_selected = test_df[test_df['pred_rank'] <= TOP_N]
            portfolio_return = top_selected[abs_return_col].mean() * 100 - ROUND_TRIP_COST * 100
    else:
        portfolio_return = q5  # fallback

    # ì—°ê°„ ì‹¤ì œ ìˆ˜ìµë¥  = ë¶„ê¸° ìˆ˜ìµë¥  ë³µë¦¬ (4íšŒ ë¦¬ë°¸ëŸ°ì‹± ê°€ì •)
    annual_portfolio = ((1 + portfolio_return/100) ** (252/HORIZON) - 1) * 100

    all_results.append({
        'year': test_year,
        'Q1': q1,
        'Q5': q5,
        'spread': spread,
        'IC': ic,
        'market_ret': market_ret,
        'abs_top': abs_top,
        'portfolio_return': portfolio_return,  # ì‹¤ì œ ë¶„ê¸° ìˆ˜ìµë¥ 
        'annual_return': annual_portfolio,     # ì‹¤ì œ ì—°í™˜ì‚° ìˆ˜ìµë¥ 
        'train_samples': len(train_df),
        'test_samples': len(test_df)
    })

    if not np.isnan(spread):
        bar = 'â–ˆ' * int(max(0, min(annual_portfolio, 50)))
        ic_status = 'âœ…' if ic > 0 else 'âŒ'
        # ëª¨ë“  ëª¨ë“œì—ì„œ ì‹¤ì œ ì—°ìˆ˜ìµë¥  í‘œì‹œ
        print(f'  {test_year}: ë¶„ê¸°={portfolio_return:+5.1f}% | ì—°í™˜ì‚°={annual_portfolio:+5.1f}% | IC={ic:+.3f} {ic_status} {bar}')

# ============================================================================
# ê²°ê³¼ ë¶„ì„
# ============================================================================
print('\n[3/3] ê²°ê³¼ ë¶„ì„')
print('=' * 70)

results_df = pd.DataFrame(all_results)

# ìµœê·¼ 5ë…„ vs ì „ì²´
recent_5y = results_df[results_df['year'] >= 2021]
all_years = results_df

print('\n[ì„±ê³¼ ìš”ì•½]')
print('-' * 50)
print(f'{"ê¸°ê°„":<15} {"í‰ê·  Spread":>12} {"í‰ê·  IC":>10} {"IC ì–‘ìˆ˜ìœ¨":>10}')
print('-' * 50)

for name, data in [('ì „ì²´', all_years), ('ìµœê·¼ 5ë…„', recent_5y)]:
    avg_spread = data['spread'].mean()
    avg_ic = data['IC'].mean()
    ic_positive = (data['IC'] > 0).sum() / len(data) * 100
    print(f'{name:<15} {avg_spread:>+11.1f}% {avg_ic:>+9.3f} {ic_positive:>9.0f}%')

print('-' * 50)

# IC ë¶„ì„
print('\n[ì—°ë„ë³„ IC]')
print('-' * 50)
for _, row in results_df.iterrows():
    ic = row['IC']
    bar = 'â–ˆ' * int(max(0, (ic + 0.1) * 100)) if ic > -0.1 else 'â–‘' * int(min(10, abs(ic) * 100))
    status = 'âœ…' if ic > 0.02 else 'âš ï¸' if ic > 0 else 'âŒ'
    print(f"  {int(row['year'])}: IC = {ic:+.3f} {status} {bar}")

# V1 vs V2 ë¹„êµ (ì˜ˆìƒ)
print('\n[V1 vs V2 ë¹„êµ]')
print('-' * 50)
print('  V1 (ì¬ë¬´ 84%): IC â‰ˆ 0, Q5 < Q1')
print(f'  V2 (ëª¨ë©˜í…€ ì¤‘ì‹¬): IC = {results_df["IC"].mean():+.3f}, Spread = {results_df["spread"].mean():+.1f}%')

# í”¼ì²˜ ì¤‘ìš”ë„
print('\n[V2 í”¼ì²˜ ì¤‘ìš”ë„ Top 15]')
print('-' * 50)

# ë§ˆì§€ë§‰ ëª¨ë¸ì˜ í”¼ì²˜ ì¤‘ìš”ë„
importance = model.feature_importance()

for i, row in importance.head(15).iterrows():
    feature = row['feature']
    imp = row['importance']

    # ê·¸ë£¹ ë¶„ë¥˜
    if feature in momentum_features:
        group = 'ëª¨ë©˜í…€'
    elif feature in volume_features:
        group = 'ìˆ˜ê¸‰'
    elif feature in volatility_features:
        group = 'ë³€ë™ì„±'
    elif feature in intuition_features:
        group = 'ë³¸ëŠ¥'
    elif feature in traditional_features:
        group = 'ì „í†µ'
    else:
        group = 'ì¬ë¬´'

    bar = 'â–ˆ' * int(imp / importance['importance'].max() * 20)
    print(f'  {feature:<25} [{group:<4}] {bar}')

# ê·¸ë£¹ë³„ ì¤‘ìš”ë„
print('\n[ê·¸ë£¹ë³„ í”¼ì²˜ ì¤‘ìš”ë„]')
print('-' * 50)

groups = {
    'ëª¨ë©˜í…€': momentum_features,
    'ìˆ˜ê¸‰': volume_features,
    'ë³€ë™ì„±': volatility_features,
    'ë³¸ëŠ¥ì „ëµ': intuition_features,
    'ì „í†µì§€í‘œ': traditional_features,
    'ì¬ë¬´': fund_features,
}

total_imp = importance['importance'].sum()
for group_name, group_features in groups.items():
    group_imp = importance[importance['feature'].isin(group_features)]['importance'].sum()
    pct = group_imp / total_imp * 100
    bar = 'â–ˆ' * int(pct / 5)
    print(f'  {group_name:<10} {pct:>5.1f}% {bar}')

# ìµœì¢… íŒì •
print('\n' + '=' * 70)
print('ğŸ’° ì‹¤ì œ í¬íŠ¸í´ë¦¬ì˜¤ ì—°ê°„ ìˆ˜ìµë¥ ')
print('=' * 70)

avg_ic = results_df['IC'].mean()
avg_spread = results_df['spread'].mean()
ic_positive_rate = (results_df['IC'] > 0).sum() / len(results_df) * 100

# ============================================================================
# í•µì‹¬: ì‹¤ì œ ì—°ê°„ ìˆ˜ìµë¥  (ì„ íƒëœ ì¢…ëª© ê¸°ë°˜)
# ============================================================================
print(f'\n  [ì—°ë„ë³„ ì‹¤ì œ ìˆ˜ìµë¥ ] (Top {TOP_N}ê°œ ì¢…ëª©, ìŠ¬ë¦¬í”¼ì§€ {args.slippage}% ë°˜ì˜)')
print('  ' + '-' * 55)
print(f'  {"ì—°ë„":<6} {"ë¶„ê¸°ìˆ˜ìµë¥ ":>12} {"ì—°í™˜ì‚°":>12} {"IC":>8}  ìƒíƒœ')
print('  ' + '-' * 55)

annual_returns = []
for _, row in results_df.iterrows():
    year = int(row['year'])
    qtr_ret = row['portfolio_return']
    ann_ret = row['annual_return']
    ic = row['IC']

    # NaN ì²´í¬ (ë¯¸ë˜ ë°ì´í„° ì—†ëŠ” ì—°ë„ ìŠ¤í‚µ)
    if pd.isna(ann_ret) or pd.isna(qtr_ret):
        continue

    annual_returns.append(ann_ret)

    # ìƒíƒœ ë°”
    if ann_ret > 20:
        bar = 'ğŸ”¥' + 'â–ˆ' * min(10, int(ann_ret / 5))
    elif ann_ret > 0:
        bar = 'âœ…' + 'â–“' * min(10, int(ann_ret / 3))
    else:
        bar = 'âŒ' + 'â–‘' * min(10, int(abs(ann_ret) / 3))

    print(f'  {year:<6} {qtr_ret:>+11.1f}% {ann_ret:>+11.1f}% {ic:>+7.3f}  {bar}')

print('  ' + '-' * 55)

# ìš”ì•½ í†µê³„
avg_annual = np.mean(annual_returns)
median_annual = np.median(annual_returns)
std_annual = np.std(annual_returns)
min_annual = np.min(annual_returns)
max_annual = np.max(annual_returns)
positive_years = sum(1 for r in annual_returns if r > 0)
total_years = len(annual_returns)

print(f'\n  ğŸ“Š ìš”ì•½ í†µê³„')
print('  ' + '-' * 55)
print(f'    í‰ê·  ì—°ìˆ˜ìµë¥ :    {avg_annual:>+8.1f}%')
print(f'    ì¤‘ì•™ê°’ ì—°ìˆ˜ìµë¥ :  {median_annual:>+8.1f}%')
print(f'    í‘œì¤€í¸ì°¨:         {std_annual:>8.1f}%')
print(f'    ìµœê³  ì—°ë„:        {max_annual:>+8.1f}%')
print(f'    ìµœì•… ì—°ë„:        {min_annual:>+8.1f}%')
print(f'    ìˆ˜ìµ ì—°ë„:        {positive_years}/{total_years}ë…„ ({positive_years/total_years*100:.0f}%)')

# í•µì‹¬ ê²°ë¡ 
print('\n  ' + '=' * 55)
if avg_annual > 15:
    emoji = 'ğŸ”¥'
elif avg_annual > 5:
    emoji = 'âœ…'
elif avg_annual > 0:
    emoji = 'âš ï¸'
else:
    emoji = 'âŒ'
print(f'  {emoji} í‰ê·  ì—°ìˆ˜ìµë¥ : {avg_annual:+.1f}% (ì¤‘ì•™ê°’: {median_annual:+.1f}%)')
print('  ' + '=' * 55)

# ëª¨ë¸ í’ˆì§ˆ ìš”ì•½

print(f'\n  [ëª¨ë¸ í’ˆì§ˆ]')
print(f'    í‰ê·  IC: {avg_ic:+.3f}')
print(f'    IC ì–‘ìˆ˜ìœ¨: {ic_positive_rate:.0f}%')

# ============================================================================
# ì•ˆì „ì„± ì²´í¬ (Safety Check)
# ============================================================================
print('\n' + '=' * 70)
print('ğŸ›¡ï¸  ì•ˆì „ì„± ì²´í¬')
print('=' * 70)

safety_checks = {}

# 1. IC ì•ˆì •ì„±
ic_stable = avg_ic >= 0.02 and ic_positive_rate >= 50
safety_checks['IC ì•ˆì •ì„± (ICâ‰¥0.02, ì–‘ìˆ˜ìœ¨â‰¥50%)'] = ic_stable

# 2. ìˆ˜ìµë¥  ì•ˆì •ì„± (ì—°í™˜ì‚° ìˆ˜ìµë¥  ë³€ë™)
ret_stable = std_annual < 25.0  # ì—°í™˜ì‚°ì´ë¼ ê¸°ì¤€ ì™„í™”
safety_checks[f'ìˆ˜ìµë¥  ì•ˆì •ì„± (Ïƒ={std_annual:.1f}% < 25%)'] = ret_stable

# 3. ì†ì‹¤ ì—°ë„ ì²´í¬
loss_years = total_years - positive_years
loss_check = loss_years <= total_years * 0.4  # 40% ì´í•˜ ì†ì‹¤ì—°ë„
safety_checks[f'ì†ì‹¤ì—°ë„ ({loss_years}/{total_years} â‰¤ 40%)'] = loss_check

# 4. ìµœì•…ì˜ í•´ ì²´í¬
worst_check = min_annual > -20.0  # ì—°í™˜ì‚° ê¸°ì¤€ -20% ì´ìƒ
safety_checks[f'ìµœì•…ì˜ í•´ ({min_annual:+.1f}% > -20%)'] = worst_check

# 5. ì—°í™˜ì‚° ìˆ˜ìµë¥  ì²´í¬
annual_check = avg_annual > 5.0  # ì—° 5% ì´ìƒ
safety_checks[f'ì—°ìˆ˜ìµë¥  ({avg_annual:+.1f}% > 5%)'] = annual_check

# 6. ìŠ¬ë¦¬í”¼ì§€ ë¯¼ê°ë„
spread_after_slip = avg_spread  # ì´ë¯¸ ìŠ¬ë¦¬í”¼ì§€ ë°˜ì˜ë¨
slip_check = spread_after_slip > 0
safety_checks[f'ìŠ¬ë¦¬í”¼ì§€ í›„ Spread ({spread_after_slip:+.1f}% > 0)'] = slip_check

# ê²°ê³¼ ì¶œë ¥
print('\n  [ì²´í¬ë¦¬ìŠ¤íŠ¸]')
print('  ' + '-' * 50)
passed = 0
for check_name, result in safety_checks.items():
    status = 'âœ… PASS' if result else 'âŒ FAIL'
    print(f'    {status}  {check_name}')
    if result:
        passed += 1

print('  ' + '-' * 50)
print(f'    í†µê³¼: {passed}/{len(safety_checks)}')

# ìµœì¢… íŒì •
print('\n  [ì „ëµ ìœ íš¨ì„± íŒì •]')
if passed == len(safety_checks):
    print('    ğŸ”¥ ë§¤ìš° ì•ˆì „ - ì‹¤ì „ íˆ¬ì… ê°€ëŠ¥')
elif passed >= len(safety_checks) * 0.7:
    print('    âœ… ì–‘í˜¸ - ì†Œì•¡ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ì¶”ì²œ')
elif passed >= len(safety_checks) * 0.5:
    print('    âš ï¸  ì£¼ì˜ - ì¶”ê°€ ê²€ì¦ í•„ìš”')
else:
    print('    âŒ ìœ„í—˜ - ì „ëµ ì¬ê²€í†  í•„ìš”')

# ì €ì¥
results_df.to_csv('backtest_v2_results.csv', index=False)
print(f'\nê²°ê³¼ ì €ì¥: backtest_v2_results.csv')

print('\n' + '=' * 70)
print('V2 ë°±í…ŒìŠ¤íŠ¸ ì™„ë£Œ!')
print('=' * 70)
